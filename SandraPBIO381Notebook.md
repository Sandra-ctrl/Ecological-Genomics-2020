# Title  

## Author: Sandra Nnadi 
### Affiliation:  PhD Student
### E-mail contact: sandra.nnadi@uvm.edu


### Start Date: 2020-01-13
### End Date: 2020-05-08
### Project Descriptions:   





# Table of Contents:   
* [Entry 1: 2020-01-13, Monday](#id-section1)
* [Entry 2: 2020-01-14, Tuesday](#id-section2)
* [Entry 3: 2020-01-15, Wednesday](#id-section3)
* [Entry 4: 2020-01-16, Thursday](#id-section4)
* [Entry 5: 2020-01-17, Friday](#id-section5)
* [Entry 6: 2020-01-20, Monday](#id-section6)
* [Entry 7: 2020-01-21, Tuesday](#id-section7)
* [Entry 8: 2020-01-22, Wednesday](#id-section8)
* [Entry 9: 2020-01-23, Thursday](#id-section9)
* [Entry 10: 2020-01-24, Friday](#id-section10)
* [Entry 11: 2020-01-27, Monday](#id-section11)
* [Entry 12: 2020-01-28, Tuesday](#id-section12)
* [Entry 13: 2020-01-29, Wednesday](#id-section13)
* [Entry 14: 2020-01-30, Thursday](#id-section14)
* [Entry 15: 2020-01-31, Friday](#id-section15)
* [Entry 16: 2020-02-03, Monday](#id-section16)
* [Entry 17: 2020-02-04, Tuesday](#id-section17)
* [Entry 18: 2020-02-05, Wednesday](#id-section18)
* [Entry 19: 2020-02-06, Thursday](#id-section19)
* [Entry 20: 2020-02-07, Friday](#id-section20)
* [Entry 21: 2020-02-10, Monday](#id-section21)
* [Entry 22: 2020-02-11, Tuesday](#id-section22)
* [Entry 23: 2020-02-12, Wednesday](#id-section23)
* [Entry 24: 2020-02-13, Thursday](#id-section24)
* [Entry 25: 2020-02-14, Friday](#id-section25)
* [Entry 26: 2020-02-17, Monday](#id-section26)
* [Entry 27: 2020-02-18, Tuesday](#id-section27)
* [Entry 28: 2020-02-19, Wednesday](#id-section28)
* [Entry 29: 2020-02-20, Thursday](#id-section29)
* [Entry 30: 2020-02-21, Friday](#id-section30)
* [Entry 31: 2020-02-24, Monday](#id-section31)
* [Entry 32: 2020-02-25, Tuesday](#id-section32)
* [Entry 33: 2020-02-26, Wednesday](#id-section33)
* [Entry 34: 2020-02-27, Thursday](#id-section34)
* [Entry 35: 2020-02-28, Friday](#id-section35)
* [Entry 36: 2020-03-02, Monday](#id-section36)
* [Entry 37: 2020-03-03, Tuesday](#id-section37)
* [Entry 38: 2020-03-04, Wednesday](#id-section38)
* [Entry 39: 2020-03-05, Thursday](#id-section39)
* [Entry 40: 2020-03-06, Friday](#id-section40)
* [Entry 41: 2020-03-09, Monday](#id-section41)
* [Entry 42: 2020-03-10, Tuesday](#id-section42)
* [Entry 43: 2020-03-11, Wednesday](#id-section43)
* [Entry 44: 2020-03-12, Thursday](#id-section44)
* [Entry 45: 2020-03-13, Friday](#id-section45)
* [Entry 46: 2020-03-16, Monday](#id-section46)
* [Entry 47: 2020-03-17, Tuesday](#id-section47)
* [Entry 48: 2020-03-18, Wednesday](#id-section48)
* [Entry 49: 2020-03-19, Thursday](#id-section49)
* [Entry 50: 2020-03-20, Friday](#id-section50)
* [Entry 51: 2020-03-23, Monday](#id-section51)
* [Entry 52: 2020-03-24, Tuesday](#id-section52)
* [Entry 53: 2020-03-25, Wednesday](#id-section53)
* [Entry 54: 2020-03-26, Thursday](#id-section54)
* [Entry 55: 2020-03-27, Friday](#id-section55)
* [Entry 56: 2020-03-30, Monday](#id-section56)
* [Entry 57: 2020-03-31, Tuesday](#id-section57)
* [Entry 58: 2020-04-01, Wednesday](#id-section58)
* [Entry 59: 2020-04-02, Thursday](#id-section59)
* [Entry 60: 2020-04-03, Friday](#id-section60)
* [Entry 61: 2020-04-06, Monday](#id-section61)
* [Entry 62: 2020-04-07, Tuesday](#id-section62)
* [Entry 63: 2020-04-08, Wednesday](#id-section63)
* [Entry 64: 2020-04-09, Thursday](#id-section64)
* [Entry 65: 2020-04-10, Friday](#id-section65)
* [Entry 66: 2020-04-13, Monday](#id-section66)
* [Entry 67: 2020-04-14, Tuesday](#id-section67)
* [Entry 68: 2020-04-15, Wednesday](#id-section68)
* [Entry 69: 2020-04-16, Thursday](#id-section69)
* [Entry 70: 2020-04-17, Friday](#id-section70)
* [Entry 71: 2020-04-20, Monday](#id-section71)
* [Entry 72: 2020-04-21, Tuesday](#id-section72)
* [Entry 73: 2020-04-22, Wednesday](#id-section73)
* [Entry 74: 2020-04-23, Thursday](#id-section74)
* [Entry 75: 2020-04-24, Friday](#id-section75)
* [Entry 76: 2020-04-27, Monday](#id-section76)
* [Entry 77: 2020-04-28, Tuesday](#id-section77)
* [Entry 78: 2020-04-29, Wednesday](#id-section78)
* [Entry 79: 2020-04-30, Thursday](#id-section79)
* [Entry 80: 2020-05-01, Friday](#id-section80)
* [Entry 81: 2020-05-04, Monday](#id-section81)
* [Entry 82: 2020-05-05, Tuesday](#id-section82)
* [Entry 83: 2020-05-06, Wednesday](#id-section83)
* [Entry 84: 2020-05-07, Thursday](#id-section84)
* [Entry 85: 2020-05-08, Friday](#id-section85)


------    
<div id='id-section1'/>   


### Entry 1: 2020-01-13, Monday.   



------    
<div id='id-section2'/>   


### Entry 2: 2020-01-14, Tuesday.   



------    
<div id='id-section3'/>   


### Entry 3: 2020-01-15, Wednesday.   



------    
<div id='id-section4'/>   


### Entry 4: 2020-01-16, Thursday.   



------    
<div id='id-section5'/>   


### Entry 5: 2020-01-17, Friday.   



------    
<div id='id-section6'/>   


### Entry 6: 2020-01-20, Monday.   



------    
<div id='id-section7'/>   


### Entry 7: 2020-01-21, Tuesday.   



------    
<div id='id-section8'/>   


### Entry 8: 2020-01-22, Wednesday.   



------    
<div id='id-section9'/>   


### Entry 9: 2020-01-23, Thursday.   



------    
<div id='id-section10'/>   


### Entry 10: 2020-01-24, Friday.   



------    
<div id='id-section11'/>   


### Entry 11: 2020-01-27, Monday.   
* Created notebook
* downloaded the template from Lauren
* copied the template
* pasted template on This PC/Documents/Github/Ecological -Genomics 2020
* committed the template
* pushed the template from Github desktop to Github browser
* viewed template on github browser
* edited the entry for Mon 27

```
cd  ~/mydata
ls
```
* edited assignment on command line tutorial

```
pwd
ls
git clone https://github.com/Sandra-ctrl/Ecological-Genomics-2020
cd Ecological-Genomics-2020/
ll
mkdir mydata
ll
mkdir myscripts
mkdir myresults
ll
cd /data/project_data/RS_ExomeSeq
ll
cd metadata
ll
cp RS_Exome_metadata.txt ~/Ecological-Genomics-2020// mydata/
cd ~/Ecological-Genomics-2020/mydata/
ll
head RS_Exome_metadata.txt
grep -w "E" RS_Exome_metadata.txt |wc
grep -w "E" RS_Exome_metadata.txt >Edge_only.txt
ls
man grep
grep -w "E" RS_Exome_metadata.txt | wc -l
grep -w "E" RS_Exome_metadata.txt |cut -f1 | uniq
grep -w "E" RS_Exome_metadata.txt |cut -f1 | uniq |wc -l
grep -w "E" RS_Exome_metadata.txt |cut -f1 | uniq >EdgePops.txt
ll
mkdir metadata
mv *txt metadata/
ll metadata/
cd metadata/
grep -w "AB" RS_Exome_metadata.txt >AB.txt
ll
rm AB.txt
ll
ll -a
vim .bashrc
cd Ecological-Genomics-2020
git pull
git add --all
git commit -m "syncing server with current repo"
git push
exit

```

------    
<div id='id-section12'/>   


### Entry 12: 2020-01-28, Tuesday.   



------    
<div id='id-section13'/>   


### Entry 13: 2020-01-29, Wednesday.   
POPGEN Day 1 Objectives
* To get background on the ecology of Red spruce (Picea rubens), and the experimental design of the exome capture data
* To understand the general work flow or “pipeline” for processing and analyzing the exome capture sequence data
* To visualize and interpret Illumina data quality (what is a fastq file; what are Phred scores?).
* To learn how to make/write a bash script, and how to use bash commands to process files in batches
* To trim the reads based on base quality scores
* To start mapping (a.k.a. aligning) each set of cleaned reads to a reference genome

difference between .Rmd and .md
.Rmd allows the ability to execute R code within R while .md allows any file on github ending with .md look nice. 
# creates a header
## increases the font
### makes font bigger
 * (space) to create bullets
 [ } to create weblink
 e.g [genomics is cool] (http://...com)
 
 embedding code with back tick
 ```
 cd ~/mydata
 ll
 ```
 will give you a code block
 
 introduction to Red Spruce. details found in tutorial.
 current red spruce species are at the limit of migration thus making them vulnerable to climate change
 Steve Keller team are studying the genetic basis of climate adaptation using exome data and a retrospective approach. 
 
# The Pipeline
Visualize, Clean, Visualize
* Visualize the quality of raw data (Program: FastQC)

* Clean raw data (Program: Trimmomatic)

* Visualize the quality of cleaned data (Program: FastQC)

Calculate #’s of cleaned, high quality reads going into mapping

Map (a.k.a. Align) cleaned reads from each sample to the reference assembly to generate sequence alignment files (Program: bwa, Input: .fastq, Output: .sam).

Remove PCR duplicates identified during mapping, and calculate alignment statistics (% of reads mapping succesully, mapping quality scores, average depth of coverage per individual)
 
```
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq
ll
zcat AB_05_R1_fastq.gz | head -n 4
```
@GWNJ-0842:368:GW1809211440:2:1101:17168:1907 1:N:0:NGAAGAGA+NTTCGCCT
GATGGGATTAGAGCCCCTGAAGGCTGATAGAACTTGAGTTTCACAGGCTCATTGCATTGAAGTGGCATTTGTGTGAATGCAGAGGAGGTACATAGGTCCTCGAGAATAAAAGAGATGTTGCTCCTCACCAAAATCAGTACAGATTATTTT
+
A<A-F<AFJFJFJA7FJJJJFFJJJJJJ<AJ-FJJ7-A-FJAJJ-JJJA7A7AFJ<FF--<FF7-AJJFJFJA-<A-FAJ<AJJ<JJF--<A-7F-777-FA77---7AJ-JF-FJF-A--AJF-7FJFF77F-A--7<-F--77<JFF<

line 1	Always begins with ‘@’ and then information about the read
line 2	The actual DNA sequence
line 3	Always begins with a ‘+’ and sometimes the same info in line 1
line 4	A string of characters which represent the quality scores; always has same number of characters as line 2

If P is the probability that a base call is an error, then:

P = 10^(–Q/10)

Q = –10 log10(P)

So:

Phred Quality Score	      Probability of incorrect base call	       Base call accuracy
10	                                1 in 10	                               90%
20	                                1 in 100	                              99%
30	                                1 in 1000	                             99.9%
40	                                1 in 10,000	                           99.99%

The Phred Q score is translated to ASCII characters so that a two digit number can be represented by a single character.

 Quality encoding: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI
                   |         |         |         |         |
    Quality score: 0........10........20........30........40 
    
```
mkdir ~/Ecological-Genomics-2020/myresults/fastqc
fastqc FILENAME.fastq.gz -o outputdirectory/
vim
for file in KOS*fastq.gz

do

 fastqc ${file} -o ~/<Ecological-Genomics-2020/myresults/fastqc

done
ll
```
to change permission on script to make it executable

```
chmod u+x fastqc.sh
./fastqc.sh
```
chmod u+x fastqc.sh    # makes the script "executable" by the "user"
./fastqc.sh             # executes the script

there was a little error with the location of the fastqc/ file. i could not locate it in the myresults folder, so i did the following

```
cd myresults
mkdir fastqc
cd fastqc
mv ~/Ecological-Genomics-2020/myscripts/fastqc/* 
ll
git pull
git add --all
git commit -m "syncing server with fastqc repo"
git push

```
# clean using trimmomatic
Copy the bash script over to your ~/myrepo/myscripts directory
Open and edit the bash script using the program vim.
Edit the file so that you’re trimming the fastq files for the population assigned to you
Change the permissions on your script to make it executable, then run it! (examples below)
cp /data/scripts/trim_loop.sh  ~/myrepo/myscripts/ 
# copies the script to your home scripts dir
vim trim_loop.sh    # open the script with vim to edit

```
cp /data/scripts/trim_loop.sh  ~/Ecological-Genomics-2020/myscripts/
vim trim_loop.sh
bash trim_loop.sh

```
This time we use the variable coding to call the name of the R1 read pair, define the name for the second read in the pair (R2), and create a basename that only contains the “pop_ind” part of the name, i.e. AB_05

    R2=${R1/_R1_fastq.gz/_R2_fastq.gz}   # defines the name for the second read in the pair (R2) based on knowing the R1 name (the file names are identifcal except for the R1 vs. R2 designation)
    f=${R1/_R1_fastq.gz/}   # creates a new variable from R1 that has the "_R1_fastq.gz" stripped off
    name=`basename ${f}`   # calls the handy "basename" function to define a new variable containing only the very last part of the filename while stripping off all the path information.  This gets us the "AB_05" bit we want.

Here’s how it should look (replace AB with your population name):

#!/bin/bash   
 
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq  

for R1 in AB*R1_fastq.gz  

do 
 
    R2=${R1/_R1_fastq.gz/_R2_fastq.gz}
    f=${R1/_R1_fastq.gz/}
    name=`basename ${f}`

    java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
        -threads 1 \
        -phred33 \
         "$R1" \
         "$R2" \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R1.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R1.cl.un.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R2.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R2.cl.un.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        MINLEN:35 
 
done 

ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
LEADING: Cut bases off the start of a read, if below a threshold quality
TRAILING: Cut bases off the end of a read, if below a threshold quality
SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
MINLEN: Drop the read if it is below a specified length

```
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/
ll /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/ | wc -l
cd ~/
exit

```




------    
<div id='id-section14'/>   


### Entry 14: 2020-01-30, Thursday.   



------    
<div id='id-section15'/>   


### Entry 15: 2020-01-31, Friday.   



------    
<div id='id-section16'/>   


### Entry 16: 2020-02-03, Monday.   



------    
<div id='id-section17'/>   


### Entry 17: 2020-02-04, Tuesday.   



------    
<div id='id-section18'/>   


### Entry 18: 2020-02-05, Wednesday.   

Learning Objectives
1.	Review our progress on read cleaning and visualizing QC
2.	Start mapping (a.k.a. aligning) each set of cleaned reads to a reference genome
3.	Visualize sequence alignment files
4.	Process our sam files by
*  converting to binary (bam) format and sorting by coordinates
*  removing PCR duplicates
*  indexing for fast future lookup
5.	Calculate mapping statistics to assess quality of the result
6.	Learn how to put separate bash scripts into a “wrapper” that runs them all

# Wget lets the server talk to the internet, a command that downloads files from the web

```
cd /data/project_data/RS_ExomeSeq/ReferenceGenomes/
wget "http://plantgenie.org/Picea_abies/v1.0/FASTA/GenomeAssemblies/Pabies1.0-genome.fa.gz"

```
Rather than trying to map to the entire 19.6 Gbp reference, we first subsetted the P. abies reference to include just the contigs that contain one or more probes from our exon capture experiment. For this, we did a BLAST search of each probe against the P. abies reference genome, and then retained all scaffolds that had a best hit.
*  This reduced reference contains:
*  668,091,227 bp (~668 Mbp) in 33,679 contigs
*  The mean (median) contig size is 10.5 (12.9) kbp
*  The N50 of the reduced reference is 101,375 bp
*  The indexed reduced reference genome to use for your mapping is on our server here: 

```
/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa

```
N50- Metric of the state of the assembly. How much of the genome is assembled into big contigs than the smaller number? How far you must keep going until you hit 334mb. Smallest contig at which the sum of the total contig. it gives you a sense of how mature the genome assembly is. Gives you more spatial information. Is the length in base pairs, so bigger number is actually better

# writing short scripts

* First, we want to specify the population of interest and the paths to the input and output directories. We can do this by defining variables in bash, like so:
* Set your repo address here – double check yours carefully!
* myrepo="/users/s/r/srkeller/Ecological_Genomics/Spring_2020"
* Each student gets assigned a population to work with: mypop="YOURPOP""
* Directory with your pop-specific demultiplexed fastq files
* input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"
* Output dir to store mapping files (bam)
* output="/data/project_data/RS_ExomeSeq/mapping"

# mypipeline.sh

#!/bin/bash

# we'll use this as a wrapper to run our different mapping scripts

# path to my repo:
myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"

# My population:

mypop="KOS"

#Directory to our cleaned and paired reads:

input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"

#Directory to store the output of our mapping
output="/data/project_data/RS_ExomeSeq/mapping"

#run mapping.sh

source ./mapping.sh

#run the post processing steps

source ./process_bam.sh


# For mapping, 
we’ll use the program bwa, which is a very efficient and very well vetted read mapper. Lots of others exist and can be useful to explore for future datasets. We tried several, and for our exome data, bwa seems to be the best
* Let’s write a bash script called mapping.sh that calls the R1 and R2 reads for each individual in our population, and uses the bwa-mem algorithm to map reads to the reference genome. We can test this out using one sample (individual) at a time, and then once the syntax is good and the bugs all worked out, we can scale this up to all the inds in our popuations. The basic bwa command we’ll use is below. Think about how we should write this into a loop to call all the fastq files for our population of interest…(hint, look back at the trim_loop.sh script)

bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
where
-t 1 is the number of threads, or computer cpus to use (in this case, just 1)
-M labels a read with a special flag if its mapping is split across >1 contig
-${ref} specifies the path and filename for the reference genome
${forward} specifies the path and filename for the cleaned and trimmed R1 reads 
${reverse} specifies the path and filename for the cleaned and trimmed R2 reads 
>${output}/BWA/${name}.sam  directs the .sam file to be saved into a directory called BWA

# script for mapping

#!/bin/bash

#this script will run the read mapping using "bwa" program

ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

#write a loop to map each individual within my population

for forward in ${input}*_R1.cl.pd.fq

do
  reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq}
  f=${forward/_R1.cl.pd.fq/}
  name=`basename ${f}`
  bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
done



# script for processing

#!/bin/bash

#this is where our output sam files are going to get converted into binary format (bam)
#then we are going to sort the bam files, remove the PCR duplicates and index them

#first, lets convert sam to bam

for f in ${output}/BWA/${mypop}*.sam

do

  out=${f/.sam/}
  sambamba-0.7.1-linux-static view -S --format=bam ${f} -o ${out}.bam
  samtools sort ${out}.bam -o ${out}.sorted.bam 
  
done

#now lets remove the PCR duplicates from our bam files

for file in ${output}/BWA/${mypop}*.sorted.bam

do

  f=${file/.sorted.bam/}
  sambamba-0.7.1-linux-static markdup -r -t 1 ${file} ${f}.sorted.rmdup.bam
  
done

```
cd Ecological-Genomics-2020/
cd myresults/
ll
cd fastqc
cd ..
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq/
ll
cd pairedcleanreads/
cd /
pwd
ll
exit 

```

```
cd Ecological-Genomics-2020/
ll /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/
# My population
mypop="KOS"
#Directory to our cleaned and paired reads
input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"
echo ${input}
output="/data/project_data/RS_ExomeSeq/mapping"
bwa
ll /data/project_data/RS_ExomeSeq/ReferenceGenomes/
pwd
ll /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
echo ${mypop}
mypop="KOS_01"
ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
#write a loop to map eac individual within my population
 for forward in ${input}*_R1.cl.pd.fq; do reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq}; f=${forward/_R1.cl.pd.fq/}; name=`basename ${f}`; bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam; done
cd myscripts/
ll
chmod u+x mapping.sh
chmod u+x mypipeline.sh
chmod u+x process_bam.sh
ll
git pull
screen
top
screen -r
exit
```
# using the wildcard

```
ll /data/project_data/RS_ExomeSeq/mapping/BWA/
ll /data/project_data/RS_ExomeSeq/mapping/BWA/KOS*
ll /data/project_data/RS_ExomeSeq/mapping/BWA/KOS*bai
ll /data/project_data/RS_ExomeSeq/mapping/BWA/KOS*.ba
ll /data/project_data/RS_ExomeSeq/mapping/BWA/KOS*bam
exit

```









------    
<div id='id-section19'/>   


### Entry 19: 2020-02-06, Thursday.   



------    
<div id='id-section20'/>   


### Entry 20: 2020-02-07, Friday.   



------    
<div id='id-section21'/>   


### Entry 21: 2020-02-10, Monday.   



------    
<div id='id-section22'/>   


### Entry 22: 2020-02-11, Tuesday.   



------    
<div id='id-section23'/>   


### Entry 23: 2020-02-12, Wednesday.   

Learning Objectives for 02/12/20
1.	Review our progress on mapping
2.	Calculate mapping statistics to assess quality of the result (how many of the reads mapped uniquely, how many reads on average cover the site (depth) 
3.	Visualize sequence alignment files
4.	Introduce use of genotype-likelihoods for analyzing diversity in low coverage sequences. Embracing statiscal uncertainty in alignment
5.	Use the ‘ANGSD’ progranm to calculate diversity stats, Fsts, and PCA
Sam file is human readeable
/data/project_data/RS_ExomeSeq/mapping/BWA/
ll
ll KOS*sam
head KOS_01.sam
tail KOS_01.sam

a row of data consists of a potential data. The first row is the reads name. the next number is a flag (e.g 147 can be known by a SAM decode- means the read was paired. It was mapped together as forward and reverse. It’s the second read in the pair. It’s the reverse read.r)
when a read maps to more than one spot its not a primary alignment
the next is the contig that the read mapped to (MA_18732) the left most position of the read. 
60 is the base quality score, 6 orders of magnitude, it’s a very strong match- mapping quality (MAQ) 
Followed by the actual sequence and the quality scores

A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference genome or transcriptome. For each read in a FASTQ file, there’s a line in the SAM file that includes
•	the read, aka. query, name,
•	a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),
•	the reference sequence name to which the read mapped
•	the leftmost position in the reference where the read mapped
•	the mapping quality (Phred-scaled)
•	a CIGAR string that gives alignment information (how many bases Match (M), where there’s an Insertion (I) or Deletion (D))
•	an ‘=’, mate position, inferred insert size (columns 7,8,9),
•	the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),
•	then Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).
The left (R1) and right (R2) reads alternate through the file. SAM files usually have a header section with general information where each line starts with the ‘@’ symbol. SAM and BAM files contain the same information; SAM is human readable and BAM is in binary code and therefore has a smaller file size.
Find the official Sequence AlignMent file documentation can be found here or more officially.
•	Here’s a SAM FLAG decoder by the Broad Institute. Use this to decode the second column of numbers
Useful for getting info on the sam files
How can we get a summary of how well our reads mapped to the reference?
•	We can use the program samtools Written by Heng Li, the same person who wrote bwa. It is a powerful tool for manipulating sam/bam files.
•	The command flagstat gets us some basic info on how well the mapping worked:
•	samtools flagstat KOS_01.sam
filename sorted.rmdup.bam
filename sorted.rmdup.bai
writing bamstats scripts
#!/bin/bash

#set repo

myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"

mypop="KOS"

output="/data/project_data/RS_ExomeSeq/mapping"

echo "num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" > ${myrepo}/myresults/${mypop}.flagstats.txt

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
do
  f=${file/.sorted.rmdup.bam/}
  name=`basename ${f}`
  echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
  samtools flagstat ${file} | awk 'NR>=6&&NR<=12 {print $1}' | column -x 
done >> ${myrepo}/myresults/${mypop}.flagstats.txt

# Calculate depth of coverage from our bam files

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
do
  samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}'
 done >> ${myrepo}/myresults/${mypop}.coverage.txt
 
 While that’s running, we can take a look at one of our alignment files (sam or bam) using an integrated viewed in samtools called tview. To use it, simply call the program and command, followed by the sam/bam file you want to view and the path to the reference genome. For example:
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/AB_05.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
Genotype-free population genetics using genotype likelihoods
A growing movement in popgen analysis of NGS data is embracing the use of genotype likelihoods to calculate stats based on each individual having a likelihood (probability) of being each genotype.
A genotype likelihood (GL) is essentially the probability of observing the sequencing data (reads containing a particular base), given the genotype of the individual at that site.
These probabilities are modeled explicitly in the calculation of population diversty stats like pi, Tajima’s D, Fst, PCA, etc…; thus not throwing out any precious data, but also making fewer assumptions about the true (unknown) genotype at each locus
•	We’re going to use this approach with the program ‘ANGSD’, which stands for ‘Analysis of Next Generation Sequence Data’
•	This approach was pioneered by Rasmus Nielsen, published originally in Korneliussen et al. 2014.

1.	Create a list of bam files for the samples you want to analyze
2.	Estimate genotype likelihoods (GL’s) and allele frequencies after filtering to minimize noise
3.	Use GL’s to:
•	estimate the site frequency spectrum (SFS)
•	estimate nucleotide diversities (Watterson’s theta, pi, Tajima’s D, …)
•	estimate Fst between all populations, or pairwise between sets of populations
•	perform a genetic PCA based on estimation of the genetic covariance matrix (this is done on the entire set of Edge ind’s)

Writing ANGSD script
myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"

mkdir ${myrepo}/myresults/ANGSD

output="${myrepo}/myresults/ANGSD"

mypop="KOS"

ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output}/${mypop}_bam.list 

REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

# Estimating GL's and allele frequencies for all sites with ANGSD

ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-doHWE 1 \
# -SNP_pval 1e-6

Baq1- Discounts snps close to  indels
Min q- individual bases have to have a small prob of being called incorrectly
Set min depth-At least 3 reads at the site in order to keep it
setMaxDepthInd- We set a max depth for individual we can accept to remove PCR duplicates that is excess. When there are duplicates of a gene. If the reads are stacking up together its not good. 
Skiptriallelic- We skip sites that have 3 or more alleles
Genotype likelihood model GL 1 
doCounts- Counts of allele at each site
Identify major (most frequent-ancestral allele) and minor allele (rare or derived allel)
doSaf 1- Generates statistical analysis of alleles



```
cd /data/project_data/RS_ExomeSeq/mapping/BWA/
ll
ll KOS*sam
head KOS_01.sam
tail KOS_01.sam
samtools flagstat KOS_01.sam
cd ~/Ecological-Genomics-2020/
pwd
/users/s/n/snnadi/Ecological-Genomics-2020
mypop+"KOS"
myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"
output="/data/project_data/RS_ExomeSeq/mapping"
echo "num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" > ${myrepo}/myresults/${mypop}.flagstats.txt
ll
cd myresults/
ll
cat KOS.flagstats.txt
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam; do  f=${file/.sorted.rmdup.bam/}; name=`basename ${f}`
echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
samtools flagstat ${file} | awk 'NR>6&&NR<=12 {print $1}' | column -x
done >> ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${output}/BWA/${mypop}*sorted.rmdup.bam
ll ${output}/BWA/${mypop}*
less ${myrepo}/myresults/${mypop}.flagstats.txt
rm ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${myrepo}/myresults/${mypop}.names.txt
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam; do  
samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}'
done >> ${myrepo}/myresults/${mypop}.coverage.txt
ll /data/project_data/RS_ExomeSeq/mapping/BWA/
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/KOS_01.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/KOS_02.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
pwd
/users/s/n/snnadi/Ecological-Genomics-2020
myrepo=/users/s/n/snnadi/Ecological-Genomics-2020
mkdir ${myrepo}/myresults/ANGSD
output="${myrepo}/myresults/ANGSD"
mypop="KOS"
ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output} /${mypop}_bam.list
ll
cd myresults
ll
cd ANGSD/
ll
cat KOS_bam.list

```






------    
<div id='id-section24'/>   


### Entry 24: 2020-02-13, Thursday.   



------    
<div id='id-section25'/>   


### Entry 25: 2020-02-14, Friday.   



------    
<div id='id-section26'/>   


### Entry 26: 2020-02-17, Monday.   



------    
<div id='id-section27'/>   


### Entry 27: 2020-02-18, Tuesday.   



------    
<div id='id-section28'/>   


### Entry 28: 2020-02-19, Wednesday.   



------    
<div id='id-section29'/>   


### Entry 29: 2020-02-20, Thursday.   



------    
<div id='id-section30'/>   


### Entry 30: 2020-02-21, Friday.   



------    
<div id='id-section31'/>   


### Entry 31: 2020-02-24, Monday.   



------    
<div id='id-section32'/>   


### Entry 32: 2020-02-25, Tuesday.   



------    
<div id='id-section33'/>   


### Entry 33: 2020-02-26, Wednesday.   



------    
<div id='id-section34'/>   


### Entry 34: 2020-02-27, Thursday.   



------    
<div id='id-section35'/>   


### Entry 35: 2020-02-28, Friday.   



------    
<div id='id-section36'/>   


### Entry 36: 2020-03-02, Monday.   



------    
<div id='id-section37'/>   


### Entry 37: 2020-03-03, Tuesday.   



------    
<div id='id-section38'/>   


### Entry 38: 2020-03-04, Wednesday.   



------    
<div id='id-section39'/>   


### Entry 39: 2020-03-05, Thursday.   



------    
<div id='id-section40'/>   


### Entry 40: 2020-03-06, Friday.   



------    
<div id='id-section41'/>   


### Entry 41: 2020-03-09, Monday.   



------    
<div id='id-section42'/>   


### Entry 42: 2020-03-10, Tuesday.   



------    
<div id='id-section43'/>   


### Entry 43: 2020-03-11, Wednesday.   



------    
<div id='id-section44'/>   


### Entry 44: 2020-03-12, Thursday.   



------    
<div id='id-section45'/>   


### Entry 45: 2020-03-13, Friday.   



------    
<div id='id-section46'/>   


### Entry 46: 2020-03-16, Monday.   



------    
<div id='id-section47'/>   


### Entry 47: 2020-03-17, Tuesday.   



------    
<div id='id-section48'/>   


### Entry 48: 2020-03-18, Wednesday.   



------    
<div id='id-section49'/>   


### Entry 49: 2020-03-19, Thursday.   



------    
<div id='id-section50'/>   


### Entry 50: 2020-03-20, Friday.   



------    
<div id='id-section51'/>   


### Entry 51: 2020-03-23, Monday.   



------    
<div id='id-section52'/>   


### Entry 52: 2020-03-24, Tuesday.   



------    
<div id='id-section53'/>   


### Entry 53: 2020-03-25, Wednesday.   



------    
<div id='id-section54'/>   


### Entry 54: 2020-03-26, Thursday.   



------    
<div id='id-section55'/>   


### Entry 55: 2020-03-27, Friday.   



------    
<div id='id-section56'/>   


### Entry 56: 2020-03-30, Monday.   



------    
<div id='id-section57'/>   


### Entry 57: 2020-03-31, Tuesday.   



------    
<div id='id-section58'/>   


### Entry 58: 2020-04-01, Wednesday.   



------    
<div id='id-section59'/>   


### Entry 59: 2020-04-02, Thursday.   



------    
<div id='id-section60'/>   


### Entry 60: 2020-04-03, Friday.   



------    
<div id='id-section61'/>   


### Entry 61: 2020-04-06, Monday.   



------    
<div id='id-section62'/>   


### Entry 62: 2020-04-07, Tuesday.   



------    
<div id='id-section63'/>   


### Entry 63: 2020-04-08, Wednesday.   



------    
<div id='id-section64'/>   


### Entry 64: 2020-04-09, Thursday.   



------    
<div id='id-section65'/>   


### Entry 65: 2020-04-10, Friday.   



------    
<div id='id-section66'/>   


### Entry 66: 2020-04-13, Monday.   



------    
<div id='id-section67'/>   


### Entry 67: 2020-04-14, Tuesday.   



------    
<div id='id-section68'/>   


### Entry 68: 2020-04-15, Wednesday.   



------    
<div id='id-section69'/>   


### Entry 69: 2020-04-16, Thursday.   



------    
<div id='id-section70'/>   


### Entry 70: 2020-04-17, Friday.   



------    
<div id='id-section71'/>   


### Entry 71: 2020-04-20, Monday.   



------    
<div id='id-section72'/>   


### Entry 72: 2020-04-21, Tuesday.   



------    
<div id='id-section73'/>   


### Entry 73: 2020-04-22, Wednesday.   



------    
<div id='id-section74'/>   


### Entry 74: 2020-04-23, Thursday.   



------    
<div id='id-section75'/>   


### Entry 75: 2020-04-24, Friday.   



------    
<div id='id-section76'/>   


### Entry 76: 2020-04-27, Monday.   



------    
<div id='id-section77'/>   


### Entry 77: 2020-04-28, Tuesday.   



------    
<div id='id-section78'/>   


### Entry 78: 2020-04-29, Wednesday.   



------    
<div id='id-section79'/>   


### Entry 79: 2020-04-30, Thursday.   



------    
<div id='id-section80'/>   


### Entry 80: 2020-05-01, Friday.   



------    
<div id='id-section81'/>   


### Entry 81: 2020-05-04, Monday.   



------    
<div id='id-section82'/>   


### Entry 82: 2020-05-05, Tuesday.   



------    
<div id='id-section83'/>   


### Entry 83: 2020-05-06, Wednesday.   



------    
<div id='id-section84'/>   


### Entry 84: 2020-05-07, Thursday.   



------    
<div id='id-section85'/>   


### Entry 85: 2020-05-08, Friday.   



