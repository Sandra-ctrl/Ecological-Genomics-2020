# Title  

## Author: Sandra Nnadi 
### Affiliation:  PhD Student
### E-mail contact: sandra.nnadi@uvm.edu


### Start Date: 2020-01-13
### End Date: 2020-05-08
### Project Descriptions:   





# Table of Contents:   
* [Entry 1: 2020-01-13, Monday](#id-section1)
* [Entry 2: 2020-01-14, Tuesday](#id-section2)
* [Entry 3: 2020-01-15, Wednesday](#id-section3)
* [Entry 4: 2020-01-16, Thursday](#id-section4)
* [Entry 5: 2020-01-17, Friday](#id-section5)
* [Entry 6: 2020-01-20, Monday](#id-section6)
* [Entry 7: 2020-01-21, Tuesday](#id-section7)
* [Entry 8: 2020-01-22, Wednesday](#id-section8)
* [Entry 9: 2020-01-23, Thursday](#id-section9)
* [Entry 10: 2020-01-24, Friday](#id-section10)
* [Entry 11: 2020-01-27, Monday](#id-section11)
* [Entry 12: 2020-01-28, Tuesday](#id-section12)
* [Entry 13: 2020-01-29, Wednesday](#id-section13)
* [Entry 14: 2020-01-30, Thursday](#id-section14)
* [Entry 15: 2020-01-31, Friday](#id-section15)
* [Entry 16: 2020-02-03, Monday](#id-section16)
* [Entry 17: 2020-02-04, Tuesday](#id-section17)
* [Entry 18: 2020-02-05, Wednesday](#id-section18)
* [Entry 19: 2020-02-06, Thursday](#id-section19)
* [Entry 20: 2020-02-07, Friday](#id-section20)
* [Entry 21: 2020-02-10, Monday](#id-section21)
* [Entry 22: 2020-02-11, Tuesday](#id-section22)
* [Entry 23: 2020-02-12, Wednesday](#id-section23)
* [Entry 24: 2020-02-13, Thursday](#id-section24)
* [Entry 25: 2020-02-14, Friday](#id-section25)
* [Entry 26: 2020-02-17, Monday](#id-section26)
* [Entry 27: 2020-02-18, Tuesday](#id-section27)
* [Entry 28: 2020-02-19, Wednesday](#id-section28)
* [Entry 29: 2020-02-20, Thursday](#id-section29)
* [Entry 30: 2020-02-21, Friday](#id-section30)
* [Entry 31: 2020-02-24, Monday](#id-section31)
* [Entry 32: 2020-02-25, Tuesday](#id-section32)
* [Entry 33: 2020-02-26, Wednesday](#id-section33)
* [Entry 34: 2020-02-27, Thursday](#id-section34)
* [Entry 35: 2020-02-28, Friday](#id-section35)
* [Entry 36: 2020-03-02, Monday](#id-section36)
* [Entry 37: 2020-03-03, Tuesday](#id-section37)
* [Entry 38: 2020-03-04, Wednesday](#id-section38)
* [Entry 39: 2020-03-05, Thursday](#id-section39)
* [Entry 40: 2020-03-06, Friday](#id-section40)
* [Entry 41: 2020-03-09, Monday](#id-section41)
* [Entry 42: 2020-03-10, Tuesday](#id-section42)
* [Entry 43: 2020-03-11, Wednesday](#id-section43)
* [Entry 44: 2020-03-12, Thursday](#id-section44)
* [Entry 45: 2020-03-13, Friday](#id-section45)
* [Entry 46: 2020-03-16, Monday](#id-section46)
* [Entry 47: 2020-03-17, Tuesday](#id-section47)
* [Entry 48: 2020-03-18, Wednesday](#id-section48)
* [Entry 49: 2020-03-19, Thursday](#id-section49)
* [Entry 50: 2020-03-20, Friday](#id-section50)
* [Entry 51: 2020-03-23, Monday](#id-section51)
* [Entry 52: 2020-03-24, Tuesday](#id-section52)
* [Entry 53: 2020-03-25, Wednesday](#id-section53)
* [Entry 54: 2020-03-26, Thursday](#id-section54)
* [Entry 55: 2020-03-27, Friday](#id-section55)
* [Entry 56: 2020-03-30, Monday](#id-section56)
* [Entry 57: 2020-03-31, Tuesday](#id-section57)
* [Entry 58: 2020-04-01, Wednesday](#id-section58)
* [Entry 59: 2020-04-02, Thursday](#id-section59)
* [Entry 60: 2020-04-03, Friday](#id-section60)
* [Entry 61: 2020-04-06, Monday](#id-section61)
* [Entry 62: 2020-04-07, Tuesday](#id-section62)
* [Entry 63: 2020-04-08, Wednesday](#id-section63)
* [Entry 64: 2020-04-09, Thursday](#id-section64)
* [Entry 65: 2020-04-10, Friday](#id-section65)
* [Entry 66: 2020-04-13, Monday](#id-section66)
* [Entry 67: 2020-04-14, Tuesday](#id-section67)
* [Entry 68: 2020-04-15, Wednesday](#id-section68)
* [Entry 69: 2020-04-16, Thursday](#id-section69)
* [Entry 70: 2020-04-17, Friday](#id-section70)
* [Entry 71: 2020-04-20, Monday](#id-section71)
* [Entry 72: 2020-04-21, Tuesday](#id-section72)
* [Entry 73: 2020-04-22, Wednesday](#id-section73)
* [Entry 74: 2020-04-23, Thursday](#id-section74)
* [Entry 75: 2020-04-24, Friday](#id-section75)
* [Entry 76: 2020-04-27, Monday](#id-section76)
* [Entry 77: 2020-04-28, Tuesday](#id-section77)
* [Entry 78: 2020-04-29, Wednesday](#id-section78)
* [Entry 79: 2020-04-30, Thursday](#id-section79)
* [Entry 80: 2020-05-01, Friday](#id-section80)
* [Entry 81: 2020-05-04, Monday](#id-section81)
* [Entry 82: 2020-05-05, Tuesday](#id-section82)
* [Entry 83: 2020-05-06, Wednesday](#id-section83)
* [Entry 84: 2020-05-07, Thursday](#id-section84)
* [Entry 85: 2020-05-08, Friday](#id-section85)


------    
<div id='id-section1'/>   


### Entry 1: 2020-01-13, Monday.   



------    
<div id='id-section2'/>   


### Entry 2: 2020-01-14, Tuesday.   



------    
<div id='id-section3'/>   


### Entry 3: 2020-01-15, Wednesday.   



------    
<div id='id-section4'/>   


### Entry 4: 2020-01-16, Thursday.   



------    
<div id='id-section5'/>   


### Entry 5: 2020-01-17, Friday.   



------    
<div id='id-section6'/>   


### Entry 6: 2020-01-20, Monday.   



------    
<div id='id-section7'/>   


### Entry 7: 2020-01-21, Tuesday.   



------    
<div id='id-section8'/>   


### Entry 8: 2020-01-22, Wednesday.   



------    
<div id='id-section9'/>   


### Entry 9: 2020-01-23, Thursday.   



------    
<div id='id-section10'/>   


### Entry 10: 2020-01-24, Friday.   



------    
<div id='id-section11'/>   


### Entry 11: 2020-01-27, Monday.   
* Created notebook
* downloaded the template from Lauren
* copied the template
* pasted template on This PC/Documents/Github/Ecological -Genomics 2020
* committed the template
* pushed the template from Github desktop to Github browser
* viewed template on github browser
* edited the entry for Mon 27

```
cd  ~/mydata
ls
```
* edited assignment on command line tutorial

```
pwd
ls
git clone https://github.com/Sandra-ctrl/Ecological-Genomics-2020
cd Ecological-Genomics-2020/
ll
mkdir mydata
ll
mkdir myscripts
mkdir myresults
ll
cd /data/project_data/RS_ExomeSeq
ll
cd metadata
ll
cp RS_Exome_metadata.txt ~/Ecological-Genomics-2020// mydata/
cd ~/Ecological-Genomics-2020/mydata/
ll
head RS_Exome_metadata.txt
grep -w "E" RS_Exome_metadata.txt |wc
grep -w "E" RS_Exome_metadata.txt >Edge_only.txt
ls
man grep
grep -w "E" RS_Exome_metadata.txt | wc -l
grep -w "E" RS_Exome_metadata.txt |cut -f1 | uniq
grep -w "E" RS_Exome_metadata.txt |cut -f1 | uniq |wc -l
grep -w "E" RS_Exome_metadata.txt |cut -f1 | uniq >EdgePops.txt
ll
mkdir metadata
mv *txt metadata/
ll metadata/
cd metadata/
grep -w "AB" RS_Exome_metadata.txt >AB.txt
ll
rm AB.txt
ll
ll -a
vim .bashrc
cd Ecological-Genomics-2020
git pull
git add --all
git commit -m "syncing server with current repo"
git push
exit

```

------    
<div id='id-section12'/>   


### Entry 12: 2020-01-28, Tuesday.   



------    
<div id='id-section13'/>   


### Entry 13: 2020-01-29, Wednesday.   
POPGEN Day 1 Objectives
* To get background on the ecology of Red spruce (Picea rubens), and the experimental design of the exome capture data
* To understand the general work flow or “pipeline” for processing and analyzing the exome capture sequence data
* To visualize and interpret Illumina data quality (what is a fastq file; what are Phred scores?).
* To learn how to make/write a bash script, and how to use bash commands to process files in batches
* To trim the reads based on base quality scores
* To start mapping (a.k.a. aligning) each set of cleaned reads to a reference genome

difference between .Rmd and .md
.Rmd allows the ability to execute R code within R while .md allows any file on github ending with .md look nice. 
# creates a header
## increases the font
### makes font bigger
 * (space) to create bullets
 [ } to create weblink
 e.g [genomics is cool] (http://...com)
 
 embedding code with back tick
 ```
 cd ~/mydata
 ll
 ```
 will give you a code block
 
 introduction to Red Spruce. details found in tutorial.
 current red spruce species are at the limit of migration thus making them vulnerable to climate change
 Steve Keller team are studying the genetic basis of climate adaptation using exome data and a retrospective approach. 
 
# The Pipeline
Visualize, Clean, Visualize
* Visualize the quality of raw data (Program: FastQC)

* Clean raw data (Program: Trimmomatic)

* Visualize the quality of cleaned data (Program: FastQC)

Calculate #’s of cleaned, high quality reads going into mapping

Map (a.k.a. Align) cleaned reads from each sample to the reference assembly to generate sequence alignment files (Program: bwa, Input: .fastq, Output: .sam).

Remove PCR duplicates identified during mapping, and calculate alignment statistics (% of reads mapping succesully, mapping quality scores, average depth of coverage per individual)
 
```
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq
ll
zcat AB_05_R1_fastq.gz | head -n 4
```
@GWNJ-0842:368:GW1809211440:2:1101:17168:1907 1:N:0:NGAAGAGA+NTTCGCCT
GATGGGATTAGAGCCCCTGAAGGCTGATAGAACTTGAGTTTCACAGGCTCATTGCATTGAAGTGGCATTTGTGTGAATGCAGAGGAGGTACATAGGTCCTCGAGAATAAAAGAGATGTTGCTCCTCACCAAAATCAGTACAGATTATTTT
+
A<A-F<AFJFJFJA7FJJJJFFJJJJJJ<AJ-FJJ7-A-FJAJJ-JJJA7A7AFJ<FF--<FF7-AJJFJFJA-<A-FAJ<AJJ<JJF--<A-7F-777-FA77---7AJ-JF-FJF-A--AJF-7FJFF77F-A--7<-F--77<JFF<

line 1	Always begins with ‘@’ and then information about the read
line 2	The actual DNA sequence
line 3	Always begins with a ‘+’ and sometimes the same info in line 1
line 4	A string of characters which represent the quality scores; always has same number of characters as line 2

If P is the probability that a base call is an error, then:

P = 10^(–Q/10)

Q = –10 log10(P)

So:

Phred Quality Score	      Probability of incorrect base call	       Base call accuracy
10	                                1 in 10	                               90%
20	                                1 in 100	                              99%
30	                                1 in 1000	                             99.9%
40	                                1 in 10,000	                           99.99%

The Phred Q score is translated to ASCII characters so that a two digit number can be represented by a single character.

 Quality encoding: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI
                   |         |         |         |         |
    Quality score: 0........10........20........30........40 
    
```
mkdir ~/Ecological-Genomics-2020/myresults/fastqc
fastqc FILENAME.fastq.gz -o outputdirectory/
vim
for file in KOS*fastq.gz

do

 fastqc ${file} -o ~/<Ecological-Genomics-2020/myresults/fastqc

done
ll
```
to change permission on script to make it executable

```
chmod u+x fastqc.sh
./fastqc.sh
```
chmod u+x fastqc.sh    # makes the script "executable" by the "user"
./fastqc.sh             # executes the script

there was a little error with the location of the fastqc/ file. i could not locate it in the myresults folder, so i did the following

```
cd myresults
mkdir fastqc
cd fastqc
mv ~/Ecological-Genomics-2020/myscripts/fastqc/* 
ll
git pull
git add --all
git commit -m "syncing server with fastqc repo"
git push

```
# clean using trimmomatic
Copy the bash script over to your ~/myrepo/myscripts directory
Open and edit the bash script using the program vim.
Edit the file so that you’re trimming the fastq files for the population assigned to you
Change the permissions on your script to make it executable, then run it! (examples below)
cp /data/scripts/trim_loop.sh  ~/myrepo/myscripts/ 
# copies the script to your home scripts dir
vim trim_loop.sh    # open the script with vim to edit

```
cp /data/scripts/trim_loop.sh  ~/Ecological-Genomics-2020/myscripts/
vim trim_loop.sh
bash trim_loop.sh

```
This time we use the variable coding to call the name of the R1 read pair, define the name for the second read in the pair (R2), and create a basename that only contains the “pop_ind” part of the name, i.e. AB_05

    R2=${R1/_R1_fastq.gz/_R2_fastq.gz}   # defines the name for the second read in the pair (R2) based on knowing the R1 name (the file names are identifcal except for the R1 vs. R2 designation)
    f=${R1/_R1_fastq.gz/}   # creates a new variable from R1 that has the "_R1_fastq.gz" stripped off
    name=`basename ${f}`   # calls the handy "basename" function to define a new variable containing only the very last part of the filename while stripping off all the path information.  This gets us the "AB_05" bit we want.

Here’s how it should look (replace AB with your population name):

#!/bin/bash   
 
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq  

for R1 in AB*R1_fastq.gz  

do 
 
    R2=${R1/_R1_fastq.gz/_R2_fastq.gz}
    f=${R1/_R1_fastq.gz/}
    name=`basename ${f}`

    java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \
        -threads 1 \
        -phred33 \
         "$R1" \
         "$R2" \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R1.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R1.cl.un.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${name}_R2.cl.pd.fq \
         /data/project_data/RS_ExomeSeq/fastq/edge_fastq/unpairedcleanreads/${name}_R2.cl.un.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        MINLEN:35 
 
done 

ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
LEADING: Cut bases off the start of a read, if below a threshold quality
TRAILING: Cut bases off the end of a read, if below a threshold quality
SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
MINLEN: Drop the read if it is below a specified length

```
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/
ll /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/ | wc -l
cd ~/
exit

```




------    
<div id='id-section14'/>   


### Entry 14: 2020-01-30, Thursday.   



------    
<div id='id-section15'/>   


### Entry 15: 2020-01-31, Friday.   



------    
<div id='id-section16'/>   


### Entry 16: 2020-02-03, Monday.   



------    
<div id='id-section17'/>   


### Entry 17: 2020-02-04, Tuesday.   



------    
<div id='id-section18'/>   


### Entry 18: 2020-02-05, Wednesday.   

Learning Objectives
1.	Review our progress on read cleaning and visualizing QC
2.	Start mapping (a.k.a. aligning) each set of cleaned reads to a reference genome
3.	Visualize sequence alignment files
4.	Process our sam files by
*  converting to binary (bam) format and sorting by coordinates
*  removing PCR duplicates
*  indexing for fast future lookup
5.	Calculate mapping statistics to assess quality of the result
6.	Learn how to put separate bash scripts into a “wrapper” that runs them all

# Wget lets the server talk to the internet, a command that downloads files from the web

```
cd /data/project_data/RS_ExomeSeq/ReferenceGenomes/
wget "http://plantgenie.org/Picea_abies/v1.0/FASTA/GenomeAssemblies/Pabies1.0-genome.fa.gz"

```
Rather than trying to map to the entire 19.6 Gbp reference, we first subsetted the P. abies reference to include just the contigs that contain one or more probes from our exon capture experiment. For this, we did a BLAST search of each probe against the P. abies reference genome, and then retained all scaffolds that had a best hit.
*  This reduced reference contains:
*  668,091,227 bp (~668 Mbp) in 33,679 contigs
*  The mean (median) contig size is 10.5 (12.9) kbp
*  The N50 of the reduced reference is 101,375 bp
*  The indexed reduced reference genome to use for your mapping is on our server here: 

```
/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa

```
N50- Metric of the state of the assembly. How much of the genome is assembled into big contigs than the smaller number? How far you must keep going until you hit 334mb. Smallest contig at which the sum of the total contig. it gives you a sense of how mature the genome assembly is. Gives you more spatial information. Is the length in base pairs, so bigger number is actually better

# writing short scripts

* First, we want to specify the population of interest and the paths to the input and output directories. We can do this by defining variables in bash, like so:
* Set your repo address here – double check yours carefully!
* myrepo="/users/s/r/srkeller/Ecological_Genomics/Spring_2020"
* Each student gets assigned a population to work with: mypop="YOURPOP""
* Directory with your pop-specific demultiplexed fastq files
* input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"
* Output dir to store mapping files (bam)
* output="/data/project_data/RS_ExomeSeq/mapping"

# mypipeline.sh

#!/bin/bash

# we'll use this as a wrapper to run our different mapping scripts

# path to my repo:
myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"

# My population:

mypop="KOS"

#Directory to our cleaned and paired reads:

input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"

#Directory to store the output of our mapping
output="/data/project_data/RS_ExomeSeq/mapping"

#run mapping.sh

source ./mapping.sh

#run the post processing steps

source ./process_bam.sh


# For mapping, 
we’ll use the program bwa, which is a very efficient and very well vetted read mapper. Lots of others exist and can be useful to explore for future datasets. We tried several, and for our exome data, bwa seems to be the best
* Let’s write a bash script called mapping.sh that calls the R1 and R2 reads for each individual in our population, and uses the bwa-mem algorithm to map reads to the reference genome. We can test this out using one sample (individual) at a time, and then once the syntax is good and the bugs all worked out, we can scale this up to all the inds in our popuations. The basic bwa command we’ll use is below. Think about how we should write this into a loop to call all the fastq files for our population of interest…(hint, look back at the trim_loop.sh script)

bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
where
-t 1 is the number of threads, or computer cpus to use (in this case, just 1)
-M labels a read with a special flag if its mapping is split across >1 contig
-${ref} specifies the path and filename for the reference genome
${forward} specifies the path and filename for the cleaned and trimmed R1 reads 
${reverse} specifies the path and filename for the cleaned and trimmed R2 reads 
>${output}/BWA/${name}.sam  directs the .sam file to be saved into a directory called BWA

# script for mapping

#!/bin/bash

#this script will run the read mapping using "bwa" program

ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

#write a loop to map each individual within my population

for forward in ${input}*_R1.cl.pd.fq

do
  reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq}
  f=${forward/_R1.cl.pd.fq/}
  name=`basename ${f}`
  bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam
done



# script for processing

#!/bin/bash

#this is where our output sam files are going to get converted into binary format (bam)
#then we are going to sort the bam files, remove the PCR duplicates and index them

#first, lets convert sam to bam

for f in ${output}/BWA/${mypop}*.sam

do

  out=${f/.sam/}
  sambamba-0.7.1-linux-static view -S --format=bam ${f} -o ${out}.bam
  samtools sort ${out}.bam -o ${out}.sorted.bam 
  
done

#now lets remove the PCR duplicates from our bam files

for file in ${output}/BWA/${mypop}*.sorted.bam

do

  f=${file/.sorted.bam/}
  sambamba-0.7.1-linux-static markdup -r -t 1 ${file} ${f}.sorted.rmdup.bam
  
done

```
cd Ecological-Genomics-2020/
cd myresults/
ll
cd fastqc
cd ..
cd /data/project_data/RS_ExomeSeq/fastq/edge_fastq/
ll
cd pairedcleanreads/
cd /
pwd
ll
exit 

```

```
cd Ecological-Genomics-2020/
ll /data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/
# My population
mypop="KOS"
#Directory to our cleaned and paired reads
input="/data/project_data/RS_ExomeSeq/fastq/edge_fastq/pairedcleanreads/${mypop}"
echo ${input}
output="/data/project_data/RS_ExomeSeq/mapping"
bwa
ll /data/project_data/RS_ExomeSeq/ReferenceGenomes/
pwd
ll /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
echo ${mypop}
mypop="KOS_01"
ref="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
#write a loop to map eac individual within my population
 for forward in ${input}*_R1.cl.pd.fq; do reverse=${forward/_R1.cl.pd.fq/_R2.cl.pd.fq}; f=${forward/_R1.cl.pd.fq/}; name=`basename ${f}`; bwa mem -t 1 -M ${ref} ${forward} ${reverse} > ${output}/BWA/${name}.sam; done
cd myscripts/
ll
chmod u+x mapping.sh
chmod u+x mypipeline.sh
chmod u+x process_bam.sh
ll
git pull
screen
top
screen -r
exit
```
# using the wildcard

```
ll /data/project_data/RS_ExomeSeq/mapping/BWA/
ll /data/project_data/RS_ExomeSeq/mapping/BWA/KOS*
ll /data/project_data/RS_ExomeSeq/mapping/BWA/KOS*bai
ll /data/project_data/RS_ExomeSeq/mapping/BWA/KOS*.ba
ll /data/project_data/RS_ExomeSeq/mapping/BWA/KOS*bam
exit

```









------    
<div id='id-section19'/>   


### Entry 19: 2020-02-06, Thursday.   



------    
<div id='id-section20'/>   


### Entry 20: 2020-02-07, Friday.   



------    
<div id='id-section21'/>   


### Entry 21: 2020-02-10, Monday.   



------    
<div id='id-section22'/>   


### Entry 22: 2020-02-11, Tuesday.   



------    
<div id='id-section23'/>   


### Entry 23: 2020-02-12, Wednesday.   

Learning Objectives for 02/12/20
1.	Review our progress on mapping
2.	Calculate mapping statistics to assess quality of the result (how many of the reads mapped uniquely, how many reads on average cover the site (depth) 
3.	Visualize sequence alignment files
4.	Introduce use of genotype-likelihoods for analyzing diversity in low coverage sequences. Embracing statiscal uncertainty in alignment
5.	Use the ‘ANGSD’ progranm to calculate diversity stats, Fsts, and PCA
Sam file is human readeable
/data/project_data/RS_ExomeSeq/mapping/BWA/
ll
ll KOS*sam
head KOS_01.sam
tail KOS_01.sam

a row of data consists of a potential data. The first row is the reads name. the next number is a flag (e.g 147 can be known by a SAM decode- means the read was paired. It was mapped together as forward and reverse. It’s the second read in the pair. It’s the reverse read.r)
when a read maps to more than one spot its not a primary alignment
the next is the contig that the read mapped to (MA_18732) the left most position of the read. 
60 is the base quality score, 6 orders of magnitude, it’s a very strong match- mapping quality (MAQ) 
Followed by the actual sequence and the quality scores

A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference genome or transcriptome. For each read in a FASTQ file, there’s a line in the SAM file that includes
•	the read, aka. query, name,
•	a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),
•	the reference sequence name to which the read mapped
•	the leftmost position in the reference where the read mapped
•	the mapping quality (Phred-scaled)
•	a CIGAR string that gives alignment information (how many bases Match (M), where there’s an Insertion (I) or Deletion (D))
•	an ‘=’, mate position, inferred insert size (columns 7,8,9),
•	the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),
•	then Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).
The left (R1) and right (R2) reads alternate through the file. SAM files usually have a header section with general information where each line starts with the ‘@’ symbol. SAM and BAM files contain the same information; SAM is human readable and BAM is in binary code and therefore has a smaller file size.
Find the official Sequence AlignMent file documentation can be found here or more officially.
•	Here’s a SAM FLAG decoder by the Broad Institute. Use this to decode the second column of numbers
Useful for getting info on the sam files
How can we get a summary of how well our reads mapped to the reference?
•	We can use the program samtools Written by Heng Li, the same person who wrote bwa. It is a powerful tool for manipulating sam/bam files.
•	The command flagstat gets us some basic info on how well the mapping worked:
•	samtools flagstat KOS_01.sam
filename sorted.rmdup.bam
filename sorted.rmdup.bai
writing bamstats scripts
#!/bin/bash

#set repo

myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"

mypop="KOS"

output="/data/project_data/RS_ExomeSeq/mapping"

echo "num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" > ${myrepo}/myresults/${mypop}.flagstats.txt

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
do
  f=${file/.sorted.rmdup.bam/}
  name=`basename ${f}`
  echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
  samtools flagstat ${file} | awk 'NR>=6&&NR<=12 {print $1}' | column -x 
done >> ${myrepo}/myresults/${mypop}.flagstats.txt

# Calculate depth of coverage from our bam files

for file in ${output}/BWA/${mypop}*sorted.rmdup.bam
do
  samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}'
 done >> ${myrepo}/myresults/${mypop}.coverage.txt
 
 While that’s running, we can take a look at one of our alignment files (sam or bam) using an integrated viewed in samtools called tview. To use it, simply call the program and command, followed by the sam/bam file you want to view and the path to the reference genome. For example:
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/AB_05.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
Genotype-free population genetics using genotype likelihoods
A growing movement in popgen analysis of NGS data is embracing the use of genotype likelihoods to calculate stats based on each individual having a likelihood (probability) of being each genotype.
A genotype likelihood (GL) is essentially the probability of observing the sequencing data (reads containing a particular base), given the genotype of the individual at that site.
These probabilities are modeled explicitly in the calculation of population diversty stats like pi, Tajima’s D, Fst, PCA, etc…; thus not throwing out any precious data, but also making fewer assumptions about the true (unknown) genotype at each locus
•	We’re going to use this approach with the program ‘ANGSD’, which stands for ‘Analysis of Next Generation Sequence Data’
•	This approach was pioneered by Rasmus Nielsen, published originally in Korneliussen et al. 2014.

1.	Create a list of bam files for the samples you want to analyze
2.	Estimate genotype likelihoods (GL’s) and allele frequencies after filtering to minimize noise
3.	Use GL’s to:
•	estimate the site frequency spectrum (SFS)
•	estimate nucleotide diversities (Watterson’s theta, pi, Tajima’s D, …)
•	estimate Fst between all populations, or pairwise between sets of populations
•	perform a genetic PCA based on estimation of the genetic covariance matrix (this is done on the entire set of Edge ind’s)

Writing ANGSD script
myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"

mkdir ${myrepo}/myresults/ANGSD

output="${myrepo}/myresults/ANGSD"

mypop="KOS"

ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output}/${mypop}_bam.list 

REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"

# Estimating GL's and allele frequencies for all sites with ANGSD

ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-doHWE 1 \
-SNP_pval 1e-6

Baq1- Discounts snps close to  indels
Min q- individual bases have to have a small prob of being called incorrectly
Set min depth-At least 3 reads at the site in order to keep it
setMaxDepthInd- We set a max depth for individual we can accept to remove PCR duplicates that is excess. When there are duplicates of a gene. If the reads are stacking up together its not good. 
Skiptriallelic- We skip sites that have 3 or more alleles
Genotype likelihood model GL 1 
doCounts- Counts of allele at each site
Identify major (most frequent-ancestral allele) and minor allele (rare or derived allel)
doSaf 1- Generates statistical analysis of alleles



```
cd /data/project_data/RS_ExomeSeq/mapping/BWA/
ll
ll KOS*sam
head KOS_01.sam
tail KOS_01.sam
samtools flagstat KOS_01.sam
cd ~/Ecological-Genomics-2020/
pwd
/users/s/n/snnadi/Ecological-Genomics-2020
mypop="KOS"
myrepo="/users/s/n/snnadi/Ecological-Genomics-2020"
output="/data/project_data/RS_ExomeSeq/mapping"
echo "num.reads R1 R2 Paired MateMapped Singletons MateMappedDiffChr" > ${myrepo}/myresults/${mypop}.flagstats.txt
ll
cd myresults/
ll
cat KOS.flagstats.txt
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam; do  f=${file/.sorted.rmdup.bam/}; name=`basename ${f}`
echo ${name} >> ${myrepo}/myresults/${mypop}.names.txt
samtools flagstat ${file} | awk 'NR>6&&NR<=12 {print $1}' | column -x
done >> ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${output}/BWA/${mypop}*sorted.rmdup.bam
ll ${output}/BWA/${mypop}*
less ${myrepo}/myresults/${mypop}.flagstats.txt
rm ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${myrepo}/myresults/${mypop}.flagstats.txt
cat ${myrepo}/myresults/${mypop}.names.txt
for file in ${output}/BWA/${mypop}*sorted.rmdup.bam; do  
samtools depth ${file} | awk '{sum+=$3} END {print sum/NR}'
done >> ${myrepo}/myresults/${mypop}.coverage.txt
ll /data/project_data/RS_ExomeSeq/mapping/BWA/
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/KOS_01.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
samtools tview /data/project_data/RS_ExomeSeq/mapping/BWA/KOS_02.sorted.rmdup.bam /data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa
pwd
/users/s/n/snnadi/Ecological-Genomics-2020
myrepo=/users/s/n/snnadi/Ecological-Genomics-2020
mkdir ${myrepo}/myresults/ANGSD
output="${myrepo}/myresults/ANGSD"
mypop="KOS"
ls /data/project_data/RS_ExomeSeq/mapping/BWA/${mypop}_*sorted.rm*.bam >${output} /${mypop}_bam.list
ll
cd myresults
ll
cd ANGSD/
ll
cat KOS_bam.list
REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
screen -s ANGSD
screen -r ANGSD
exit

```

------    
<div id='id-section24'/>   


### Entry 24: 2020-02-13, Thursday.   



------    
<div id='id-section25'/>   


### Entry 25: 2020-02-14, Friday.   



------    
<div id='id-section26'/>   


### Entry 26: 2020-02-17, Monday.   



------    
<div id='id-section27'/>   


### Entry 27: 2020-02-18, Tuesday.   



------    
<div id='id-section28'/>   


### Entry 28: 2020-02-19, Wednesday.   

# Learning Objectives for 02/19/20

1.	Appreciate the difference between the unfolded and the folded SFS (which should we use?)
2.	Calculate diversity stats for our focal pops (SFS, Num sites, Frequency of SNPs, theta-W, pi, Tajima’s D)
3.	Visualize results in R and share to google drive
4.	Introduce Genome-Wide Association Studies (GWAS) in ANGSD using genotype probabilities
5.	Do GWAS on seedling heights

The unfolded vs. folded SFS
The big difference here is whether we are confident in the ancestral state of each variable site (SNP) in our dataset
If we know the anestral state, then the best info is contained in the unfolded spectrum, which shows the frequency histogram of how many derived loci are rare vs. common
•	bins in the unfolded spectra go from 0 to 2N – why?
When you don’t know the ancestral state confidently, you can make the SFS based on the minor allele (the less frequent allele; always < 0.5 in the population).
•	bins in the folded spectra go from 0 to 1N – why?
Essentially, the folded spectra wraps the SFS around such that high frequency “derived” alleles are put in the small bins (low minor allele freq).

# Calculate SFS and diversity stats
In your myscripts folder, let’s revise the script ANGSD_mypop.sh to work on the folded SFS
The first part will be identical as last time, except: 1. Let’s change the -out name to -out ${output}/${mypop}_outFold 2. Take out the HWE test (not necessary to run again) and replace it with fold 1
For info on using the folded spectrum, see the ANGSD manual page for Theta stats under “folded”.
The above command generated the site allele frequencies, which we can now use to estimate the folded SFS for your pop
realSFS ${output}/${mypop}_outFold.saf.idx -maxIter 1000 -tole 1e-6 -P 1 > ${output}/${mypop}_outFold.sfs
Once you have the SFS, you can estimate the theta diversity stats:
We do this by running ANGSD again, as above, but this time we include the -pest flag which uses the SFS we estimated previously as a prior on the allele frequency estimation. We also include the doThetas flag, and of course the fold 1 to tell ANGSD we want the diversities based on the folded SFS
So, we’re copying our previous set of ANGSD commands to make a new entry and adding:
-pest ${output}/${mypop}_outFold.sfs \
-doSaf 1 \
-doThetas 1 \
-fold 1

thetaStat do_stat ${output}/${mypop}_outFold.thetas.idx
The first column of the results file (${mypop}.thetas.idx.pestPG) is formatted a bit funny and we don’t really need it. We can use the cut command to get rid of it if we want to, or just ignore it.
cut -f2- ${mypop}.thetas.idx.pestPG > ${mypop}.thetas

```
myrepo=/users/s/n/snnadi/Ecological-Genomics-2020
output="${myrepo}/myresults/ANGSD"
mypop="KOS"
REF="/data/project_data/RS_ExomeSeq/ReferenceGenomes/Pabies1.0-genome_reduced.fa"
ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_folded_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-fold 1 
realSFS ${output}/${mypop}_folded_allsites.saf.idx \
-maxIter 1000 -tole 1e-6 -P 1 \
> ${output}/${mypop}_outFold.sfs
ANGSD -b ${output}/${mypop}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${output}/${mypop}_folded_allsites \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-setMinDepth 3 \
-minInd 2 \
-setMinDepthInd 1 \
-setMaxDepthInd 17 \
-skipTriallelic 1 \
-GL 1 \
-doCounts 1 \
-doMajorMinor 1 \
-doMaf 1 \
-doSaf 1 \
-fold 1 \
-pest ${output}/${mypop}_outFold.sfs \
-doThetas 1
thetaStat do_stat ${output}/${mypop}_folded_allsites.thetas.idx

```

This is now ready to bring into R (transfer via git) to look at the mean per-site nucleotide diversity for your focal population. How does it compare to other populations? Also bring in your SFS for plotting, and to calculate the SNP frequency in your population.

# RSTUDIO SCRIPTS

setwd("~/GitHub/Ecological-Genomics-2020/myresults")
list.files()

SFS <- scan("KOS_outFold.sfs")

sumSFS <- sum(SFS)

sumSFS

pctPoly = 100*(1-(SFS[1]/sumSFS))

plotSFS <- SFS[-c(1,length(SFS))]

barplot(plotSFS)

div <-read.table("KOS_folded_allsites.thetas.idx.pestPG")

colnames(div) =c("window","chrname","winscenter","tW","tP","tF","tH","tL","tajD","fulif","fuliD","fayH","zengsE","numSites")

div$tWpersite = div$tW/div$numSites
div$tPpersite = div$tP/div$numSites

pdf("KOS_diversity_stats.pdf")

# Share your population specific stats with the group






------    
<div id='id-section29'/>   


### Entry 29: 2020-02-20, Thursday.   



------    
<div id='id-section30'/>   


### Entry 30: 2020-02-21, Friday.   



------    
<div id='id-section31'/>   


### Entry 31: 2020-02-24, Monday.   



------    
<div id='id-section32'/>   


### Entry 32: 2020-02-25, Tuesday.   



------    
<div id='id-section33'/>   


### Entry 33: 2020-02-26, Wednesday.   
# TRANSCRIPTOMICS
# Learning Objectives for 2/26/20
* Review Red Spruce ecology and biogeography and the transcriptomics experimental design.
* Articulate the questions we can address and hypotheses we can test with this experimental design.
* Understand the general work flow or “pipeline” for processing and analyzing RNAseq data.
* Review how to make/write a bash script and how to write a script to process files in batches.
* Visualize and interpret Illumina data quality (Run FastQC on raw and cleaned reads).
* Start mapping reads and quantifying abundance simultaneously using Salmon.
* Import quant.sf files generated by Salmon into DESeq2 for analysis and visualization.
* Add to your growing list of bioinformatics tricks (take notes!).

Red spruce is a montane coniferous tree that inhabits cool moist habitats in the northeast. Occasionally it also less ideal sites that are warmer and drier, particularly driven by microsite differences in aspect and substrate (rocky scree slopes exposed to direct sun). The purpose of this experiment was to sample red spruce genetic variation from sites that were polarized into cool & wet vs. hot and dry based on historic climate (based on Bio18 (precip of warmest quarter) and Bio5 (temp of warmest month), and to assess the gene expression responses of individuals from these habitats in response to experimental treatments of heat and heat+drought.

# Experimental Design:
## Ten maternal families total; sample labels are “POP_FAM”
* ASC_06, BRU_05, ESC_01, XBM_07, NOR_02, CAM_02, JAY_02, KAN_04, LOL_02, MMF_13
# Two Source Climates (SourceClim:
* HotDry (5 fams): ASC_06, BRU_05, ESC_01, XBM_07, NOR_02
* CoolWet (5 fams): CAM_02, JAY_02, KAN_04, LOL_02, MMF_13
# Experimental Treatments (Trt):
* Control: watered every day, 16:8 L:D photoperiod at 23C:17C temps
* Heat: 16:8 L:D photoperiod at 35C:26C temps (50% increase in day and night temps over controls)
* Heat+Drought: Heat plus complete water witholding
# Three time periods (Day):
* Harvested tissues on Days 0, 5, and 10
* Extracted RNA from whole seedlings (root, stem, needle tissue)
* Aimed for 5 biological reps per Trt x SourceClim x Day combo, but day5 had few RNA extractions that worked

# What questions can we ask/address with this experimental design, with these data? How do we translate these questions to models in DESeq2?
* Do families from different source climates differ for their gene expression?

* Is there a transcriptome wide response to heat stress? Does this change when the additional stress of drought is added?

* Is there a significant interaction between source climate and stress treatment, such that families from hot/dry climates have a unique expression response to heat or heat+drought compared to families from cool/wet climates?

* Which specific genes are involved in the above responses, and do they reveal functional enrichment of particular pathways?

* Do DE genes or pathways show evidence of positive selection (compare DE genes with popgen signatures of sel’n)?

* Can we use association mapping to identify eQTL associated with DE responses?

# Library prep and sequencing
* Samples were quantified for RNA concentration and quality on the Bioanalyzer
* Samples >1 ng/ul were sent to Cornell for 3’ tag sequencing
* Library prep followed the LexoGen protocol and sequencing was on 1 lane of a NextSeq500 (1x86 bp reads)
* Samples were demultiplexed and named according to the convention: POP_FAM_TRT_DAY

# Data Processing Pipeline:
* FastQC on raw reads –> Trimmomatic (done!) –> FastQC on cleaned reads
* Reference transcriptome: /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies1.0-all-cds.fna.gz Downloaded from Congenie.org
* 66,632 unigenes, consisting of 26,437 high-confidence gene models, 32,150 medium-confidence gene models, and 8,045 low-confidence gene models
* Use Salmon to simulateously map reads and quantify abundance.
* Import the data into DESeq2 in R for data normalization, visualization, and statistical tests for differential gene expression.

# Choose samples to visualize for quality (FastQC) and to map (Salmon)
* Ignoring day for the moment, there are 10 PopulationByFamily groups (POP_XX) and three treatment groups (C, H, and D), so 30 groups. If you each take two of these pop by treatment groups to process, we’ll have all the samples covered. cd into the /data/project_data/RS_RNAseq/fastq directory to view the files. Let’s write on the board each of our chosen groups.

```
cd /data/project_data/RS_RNASeq/fastq/
ll
cd ~
ll
cd Ecological-Genomics-2020/
cd myscripts/
ls
cp fastqc.sh fastqcT.sh
ls
cat fastqcT.sh
vim
```
* fastqcT for visualizing raw data

```
#! /bin/bash
cd ~/Ecological-Genomics-2020/myresults/
# i am creating a new directory to store my results
mkdir fastqcT
for file in /data/project_data/RS_RNASeq/fastq/JAY_02_H*fastq.gz
do
fastqc ${file} -o fastqcT/
done
```

```
vim fastqcT.sh
bash fastqcT.sh
git pull
git add -all
git commit -m "syncing RNASeq data to repo"
git push
```


# Clean the reads with Trimmomatic

```
#!/bin/bash

cd /data/project_data/RS_RNASeq/fastq/

########## Trimmomatic for single end reads

for R1 in *R1.fastq.gz  

do 
    echo "starting sample ${R1}"
    f=${R1/_R1.fastq.gz/}
    name=`basename ${f}`

    java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticSE \
        -threads 1 \
        -phred33 \
         "$R1" \
         /data/project_data/RS_RNASeq/fastq/cleanreads/${name}_R1.cl.fq \
        ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-SE.fa:2:30:10 \
        LEADING:20 \
        TRAILING:20 \
        SLIDINGWINDOW:6:20 \
        HEADCROP:12 \
        MINLEN:35 
     echo "sample ${R1} done"
done 

```
# Run FastQC on the clean reads to confirm improvement

* fastqcTT for visualizing cleaned reads

```
cp fastqcT.sh fastqcTT.sh
ll
vim

#! /bin/bash
cd ~/Ecological-Genomics-2020/myresults/
# i am creating a new directory to store my results
mkdir fastqcTT
for file in /data/project_data/RS_RNASeq/fastq/cleanreads/JAY_02_H*.cl.fq
do
fastqc ${file} -o fastqcTT/
done

#! /bin/bash
cd ~/Ecological-Genomics-2020/myresults/
# i am creating a new directory to store my results
mkdir fastqcTT
for file in /data/project_data/RS_RNASeq/fastq/cleanreads/KAN_04_C*.cl.fq
do
fastqc ${file} -o fastqcTT/
done

vim fastqcTT.sh
bash fastqcTT.sh
git pull
git add -all
git commit -m "syncing cleanreads data to repo"
git push
```



------    
<div id='id-section34'/>   


### Entry 34: 2020-02-27, Thursday.   



------    
<div id='id-section35'/>   


### Entry 35: 2020-02-28, Friday.   



------    
<div id='id-section36'/>   


### Entry 36: 2020-03-02, Monday.   



------    
<div id='id-section37'/>   


### Entry 37: 2020-03-03, Tuesday.   



------    
<div id='id-section38'/>   


### Entry 38: 2020-03-04, Wednesday.   

# Learning Objectives for 3/4/20
* You should have installed the relevant programs in R.
* Map cleaned reads and quantify abundance simultaneously using Salmon.
* Assess mapping rate (Salmon log files); explore mapping to different reference trancript sets.
* Generate compiled counts matrix (all 76 samples) from individual quant.sf files using tximport.
* Move the data matrix to your machine (Fetch/WinSCP/SCP).
* Import data matrix and sample information into R and DESeq2
* Normalize, visualize and analyze expression data using DESeq2.

# Use Salmon to quantify transcript abundance
* First step: Index the reference transcriptome. This only needs to be done once and has been done already, but here’s the code:

```
cd /data/project_data/RS_RNAseq/ReferenceTrancriptome/
salmon index -t Pabies1.0-all-cds.fna.gz -i Pabies_cds_index --decoys decoys.txt -k 31
```
Note that the -k flag sets the minimum acceptable length for a valid match between query (read) and the reference. This is a parameter that the creators of Salmon suggest can be made “slightly smaller” if the mapping rate is lower than expected. We may want to explore reducing -k.

* Second step: Start quantification! Below is the basic command for running the quantification from the documentation, Salmon tutorial How do we turn this into a for loop to process our samples? Let’s do it.

```
salmon quant -i transcripts_index -l <LIBTYPE> -r reads.fq --validateMappings -o transcripts_quant
```
The descriptions of all of the options can be found on the Salmon github page and by using the command salmon quant -h.

```
!#/bin/bash

cd /data/project_data/RS_RNASeq/fastq/cleanreads/

for file in JAY_02_H*.cl.fq

  do

    salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC27_index \
	-l A \
	-r /data/project_data/RS_RNASeq/fastq/cleanreads/${file} \
	--validateMappings \
	-o /data/project_data/RS_RNASeq/salmon/cleanedreads/${file} 

  done 
  
  cd /data/project_data/RS_RNASeq/fastq/cleanreads/

for file in JAY_02_H*.cl.fq

  do

    salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_HC_index \
	-l A \
	-r /data/project_data/RS_RNASeq/fastq/cleanreads/${file} \
	--validateMappings \
	-p 1 \
	--seqBias \
	-o /data/project_data/RS_RNASeq/salmon/HCmapping/${file} 

  done 
  
  
  cd /data/project_data/RS_RNASeq/fastq/cleanreads/

for file in JAY_02_H*.cl.fq

  do

    salmon quant -i /data/project_data/RS_RNASeq/ReferenceTranscriptome/Pabies_cds_index \
	-l A \
	-r /data/project_data/RS_RNASeq/fastq/cleanreads/${file} \
	--validateMappings \
	-p 1 \
	--seqBias \
	-o /data/project_data/RS_RNASeq/salmon/allmapping/${file} 

  done 
  ```

# Explore mapping rate
For each sample mapped, you now have a directory with several output files including a log of the run. In that log, the mapping rate (% of reads mapped with sufficient quality) is reported. We can view the contents of the file using cat. We can also use grep (i.e., regular expressions) to pull out the mapping rate for all the samples. Though there’s probably a more elegant solution, here is one:

```
cd /data/project_data/RS_RNASeq/salmon/cleanreads/
cd /data/project_data/RS_RNASeq/salmon/allmapping/${file} 
grep -r --include \*.log -e 'Mapping rate'
cd JAY_02_C_10_TATGTC_R1.cl.fq
ll
head -n 10 quant.sf

```

------    
<div id='id-section39'/>   


### Entry 39: 2020-03-05, Thursday.   



------    
<div id='id-section40'/>   


### Entry 40: 2020-03-06, Friday.   



------    
<div id='id-section41'/>   


### Entry 41: 2020-03-09, Monday.   



------    
<div id='id-section42'/>   


### Entry 42: 2020-03-10, Tuesday.   



------    
<div id='id-section43'/>   


### Entry 43: 2020-03-11, Wednesday.   

# SPRING BREAK
------    
<div id='id-section44'/>   


### Entry 44: 2020-03-12, Thursday.   



------    
<div id='id-section45'/>   


### Entry 45: 2020-03-13, Friday.   



------    
<div id='id-section46'/>   


### Entry 46: 2020-03-16, Monday.   



------    
<div id='id-section47'/>   


### Entry 47: 2020-03-17, Tuesday.   



------    
<div id='id-section48'/>   


### Entry 48: 2020-03-18, Wednesday.   

# Learning Objectives for 3/17/20
* Recap on Mapping of 3’ RNA-seq data to transcriptome and assembly of data matrix
* Import data matrix and sample information into R and DESeq2
* Normalize, visualize and analyze expression data using DESeq2.

# Troubleshooting and improving mapping rate
two weeks ago we wrote for loops to map our set of cleaned fastq files to the reference transcriptome. We discovered that we had low mapping rates, ~2%! We did some troubleshooting in class. We further hypothesized that many of our reads were not mapping because the reference we had selected included only the coding region. In working with 3’ RNAseq data, much of our sequencing effort is likely to be in the 3’ UTR (untranslated region). We concatenated the reference sequences for the coding (“cds”) and the 3’ UTR (“2kb downstream”) for each gene. We then mapped to this new reference using salmon (as you had done before). Our mapping rate improved dramatically, ranging from 40-70% of reads mapping across samples, mean of 52%.

As described in the last tutorial, after all samples have been mapped to the refenence, the next step is to assemble the counts matrix using tximport in R on the server to be able to move from read mapping with Salmon to differential gene expression (DGE) analysis with DESeq2. See previous tutorial for code to assemble counts matrix.

# Import Counts Matrix and Sample ID tables into R and DESeq2
Now we will work in R on our individual machines, each of us working with the complete data set (n=76, not just a subset of samples).

Here’s a link to our zipped counts matrix and sample ID table.

Below is the scaffold (and a bit of code) for what we will be live coding today. Copy this into a new R document in RStudio. The idea is for you to not have to simultaneously annotate and accurately live code. Hopefully, this will also help you to better understand each step we do. Do feel free to add more notes and duplicate and experiment with scripts.

# Set your working directory
* go to session on Rstudio, click on 'set working directory' then choose 'RS_counts_samples'

# Import the libraries that we're likely to need in this session
library(DESeq2)
library(dplyr)
library(tidyr)
library(ggplot2)
library(scales)
library(ggpubr)
library(wesanderson)
library(vsn)  ### First: BiocManager::install("vsn") AND BiocManager::install("hexbin")

# Import the counts matrix
countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable) * Need to round because DESeq wants only integers
head(countsTableRound)

# Import the samples description table - links each sample to factors of the experimental design.
* Need the colClasses otherwise imports "day" as numeric which DESeq doesn't like, could alternatively change to d0, d5, d10
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)

# Let's see how many reads we have from each sample:
colSums(countsTableRound)
mean(colSums(countsTableRound))
barplot(colSums(countsTableRound), las=3, cex.names=0.5,names.arg = substring(colnames(countsTableRound),1,13))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd =2)

# Create a DESeq object and define the experimental design here with the tilde
# Filter out genes with few reads
# Run the DESeq model to test for differential gene expression: 1) estimate size factors (per sample), 2) estimate dispersion (per gene), 3) run negative binomial glm
# List the results you've generated
# Order and list and summarize results from specific contrasts
# Here you set your adjusted p-value cutoff, can make summary tables of the number of genes differentially expressed (up- or down-regulated) for each contrast

##### Data visualization #####
# MA plot
# PCA
# Counts of specific top gene! (important validatition that the normalization, model is working)
# Heatmap of top 20 genes sorted by pvalue


```
countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable)
head(countsTableRound)
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)
grep("10", names(countsTableRound), value = TRUE)
day10countstable <- subset(countsTableRound, grep("10", names(countsTableRound), value = TRUE)) #doesn't work has to be logical

day10countstable <- countsTableRound %>% select(contains("10"))
dim(day10countstable)

conds10<- subset(conds, day=="10")
dim(conds10)
head(conds10)

colSums(countsTableRound)
mean(colSums(countsTableRound))
barplot(colSums(countsTableRound), las=3, cex.names=0.5,names.arg = substring(colnames(countsTableRound),1,13))
abline(h=mean(colSums(countsTableRound)), col="blue", lwd =2)

rowSums(countsTableRound)
mean(rowSums(countsTableRound))
median(rowSums(countsTableRound))

apply(countsTableRound,2,mean)

dds <- DESeqDataSetFromMatrix(countData = countsTableRound, colData = conds, 
                              design = ~ climate + day + treatment)
dim(dds)
dds <- dds[rowSums(counts(dds)) > 76]
dim(dds)

dds <- DESeq(dds)

resultsNames(dds)

res <- results(dds, alpha = 0.05)
res <- res[order(res$padj),]
head(res)

summary(res)

res_treatCD <- results(dds, name="treatment_D_vs_C", alpha=0.05)
res_treatCD <- res_treatCD[order(res_treatCD$padj),]
head(res_treatCD)
summary(res_treatCD)

plotMA(res_treatCD,ylim=c(-3,3))

vsd <- vst(dds, blind=FALSE)

data <- plotPCA(vsd,intgroup=c("climate","treatment","day"),returnData=TRUE)
percentVar <- round(100 * attr(data, "percentVar"))

data$treatment <- factor(data$treatment, levels=c("C","H","D"), labels = c("C","H","D"))
data$day <- factor(data$day, levels=c("0","5","10"), labels = c("0","5","10"))

ggplot(data, aes(PC1, PC2, color=day, shape=treatment)) +
  geom_point(size=4, alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()

d <-plotCounts(dds, gene="MA_7017g0010", intgroup = (c("treatment","climate","day")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=day, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.3,h=0), size=3) +
  scale_x_discrete(limits=c("C","H","D"))
p

p <-ggplot(d, aes(x=treatment, y=count, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p

library(pheatmap)
topgenes <- head(rownames(res_treatCD),20)
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds)[,c("treatment","climate","day")])
pheatmap(mat, annotation_col=df)

```

# Ways to explore data analysis
* Explore the various results for a given experimental design, see: resultsNames(dds)
* Set up your DESeq object with different experimental designs; e.g., design = ~ climate + treatment + climate:treatment
* Subset your data to exclude/include different factors, run different designs; select and subset are handy functions in R for          subsetting your countsMatrix or conds table, e.g., use only the day 10 data.

# Analysis of DAY 10 samples

```
countsTable <- read.table("RS_cds2kb_countsMatrix.txt", header=TRUE, row.names=1)
head(countsTable)
dim(countsTable)
countsTableRound <- round(countsTable)
head(countsTableRound)
conds <- read.delim("RS_samples.txt", header=TRUE, stringsAsFactors = TRUE, row.names=1, colClasses=c('factor', 'factor', 'factor', 'factor'))
head(conds)
dim(conds)

grep("10", names(countsTableRound), value = TRUE)
day10countstable <- subset(countsTableRound, grep("10", names(countsTableRound), value = TRUE)) #doesn't work has to be logical

day10countstable <- countsTableRound %>% select(contains("10"))
dim(day10countstable)

conds10<- subset(conds, day=="10")
dim(conds10)
head(conds10)

colSums(day10countstable)
mean(colSums(day10countstable))
barplot(colSums(day10countstable), las=3, cex.names=0.5,names.arg = substring(colnames(day10countstable),1,13))
abline(h=mean(colSums(day10countstable)), col="blue", lwd =2)

rowSums(day10countstable)
mean(rowSums(day10countstable))
median(rowSums(day10countstable))

apply(day10countstable,2,mean)

dds <- DESeqDataSetFromMatrix(countData = day10countstable, colData = conds10, 
                              design = ~ climate + treatment + climate:treatment)
dim(dds)

dds <- dds[rowSums(counts(dds)) > 30]
dim(dds)

dds <- DESeq(dds)

resultsNames(dds)

res_treatCD <- results(dds, name="treatment_D_vs_C", alpha=0.05)
res_treatCD <- res_treatCD[order(res_treatCD$padj),]
head(res_treatCD)
summary(res_treatCD)
plotMA(res_treatCD,ylim=c(-3,3))

res_treatCH <- results(dds, name="treatment_H_vs_C", alpha = 0.05)
res_treatCH <- res_treatCH[order(res_treatCH$padj),]
head(res_treatCH)
summary(res_treatCH)
plotMA(res_treatCH,ylim=c(-3,3))

res_interClimTreat <- results(dds, name="climateHD.treatmentD", alpha=0.05)
res_interClimTreat <- res_interClimTreat[order(res_interClimTreat$padj),]
head(res_interClimTreat)
summary(res_interClimTreat)
plotMA(res_interClimTreat,ylim=c(-3,3))

res_interClimTreatH <- results(dds, name="climateHD.treatmentH", alpha=0.05)
res_interClimTreatH <- res_interClimTreatH[order(res_interClimTreatH$padj),]
head(res_interClimTreatH)
summary(res_interClimTreatH)
plotMA(res_interClimTreatH,ylim=c(-3,3))

res_ClimTreat <- results(dds, name="climate_HD_vs_CW", alpha=0.05)
res_ClimTreat <- res_ClimTreat[order(res_ClimTreat$padj),]
head(res_ClimTreat)
summary(res_ClimTreat)
plotMA(res_ClimTreat,ylim=c(-3,3))

res_Int <- results(dds, name="intercept", alpha = 0.05)
res_Int <- res_Int[order(res_Int$padj),]
head(res_Int)
summary(res_Int)
plotMA(res_Int,ylim=c(-3,3))

vsd <- vst(dds, blind=FALSE)

data <- plotPCA(vsd,intgroup=c("climate","treatment"),returnData=TRUE)
percentVar <- round(100 * attr(data, "percentVar"))

data$treatment <- factor(data$treatment, levels=c("C","H","D"), labels = c("Control","Hot","Dry+Hot"))
data$climate <- factor(data$climate, levels=c("HD","CW"), labels = c("Hot-Dry","Cold-Wet"))

ggplot(data, aes(PC1, PC2, color=climate, shape=treatment)) +
  geom_point(size=4, alpha=0.85) +
  xlab(paste0("PC1: ",percentVar[1],"% variance")) +
  ylab(paste0("PC2: ",percentVar[2],"% variance")) +
  theme_minimal()

d <-plotCounts(dds, gene="MA_133272g0010", intgroup = (c("treatment","climate")), returnData=TRUE)
d

p <-ggplot(d, aes(x=treatment, y=count, color=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p <- p + geom_point(position=position_jitter(w=0.3,h=0), size=3) +
  scale_x_discrete(limits=c("C","H","D"))
p

p <-ggplot(d, aes(x=treatment, y=count, shape=climate)) + 
  theme_minimal() + theme(text = element_text(size=20), panel.grid.major=element_line(colour="grey"))
p

library(pheatmap)
topgenes <- head(rownames(res_treatCD),20)
mat <- assay(vsd)[topgenes,]
mat <- mat - rowMeans(mat)
df <- as.data.frame(colData(dds)[,c("treatment","climate")])
pheatmap(mat, annotation_col=df)

```

# Export data to perform additional analyses
You can use the normalized counts data or the results p-values from your tests of interest for a number of downstream analyses such as functional enrichment or correlation network analyses. Below is some example code for exporting your data for various purposes:

# Export counts to use at the input for WGCNA

norm.counts <- counts(dds, normalized=TRUE)
dim(norm.counts)

write.csv(norm.counts, file = "RS_norm_counts.csv", row.names=T, quote=F)

# Write out DGE results

write.csv(res_treatCD, file="DGE_res_treatCD.csv", row.names = T, quote=F)

# Format for GOMWU: change pvalue to -log(pvalue) and export as .csv with rownames

neglogpval <- as.matrix(-log(res_treatCD$pvalue))
head(neglogpval)

res_treatCD_negpval <- cbind(row.names(res_treatCD),neglogpval)
head(res_pop_negpval)

colnames(res_treatCD_negpval)=c("gene","neglogpval")

write.csv(res_treatCD_negpval, file="DGE_treatCvD_neglogpval.csv", row.names=F,quote=F)


------    
<div id='id-section49'/>   


### Entry 49: 2020-03-19, Thursday.   



------    
<div id='id-section50'/>   


### Entry 50: 2020-03-20, Friday.   



------    
<div id='id-section51'/>   


### Entry 51: 2020-03-23, Monday.   



------    
<div id='id-section52'/>   


### Entry 52: 2020-03-24, Tuesday.   



------    
<div id='id-section53'/>   


### Entry 53: 2020-03-25, Wednesday.   

# EPIGENETICS -SPECIAL TOPIC
Learning objectives
* Background on the copepod selection experimental design
* Think about the hypotheses we can address with this experiment
* Cover how bisulfite sequencing data differs from regular data and not to panic when you see your fastqc output
* Begin mapping using Bismark

# Copepod selection experiment (The copepod, Acartia tonsa)
Acartia tonsa is a calanoid copepod that has a world wide distribution. It inhabits estuaries and coastal waters and it typically the most abundant zooplankton present. Because of their huge population sizes and global distribution, they play an important role in ocean biogeochemical cycling and ecosystem functioning. For example, they’re an important food base for many fishes. Given their broad distribution in dynamic habitats (estuaries), they can tolerate and live in a broad range of salinities, freshwater to seawater, and temperatures, among other stressors.

We know that the world is rapidly changing due to human activities and it is important to understand how species will respond to these changes. We were interested in understanding how A. tonsa could respond to stressful environments that will likely be caused by climate change. Can they adapt to rapid changes in temperature and acidity? How might epigenetic responses interact with adaptation to enable resilience?

A. tonsa is a great system to ask these questions for a number of reasons. First, their generation time is only ~10-20 days and they’re tiny! Only 1-2 mm. This means we can easily execute multi-generational studies with thousands of individuals. Second, because they can tolerate such variable environments, they should have lots of plasticity to respond to environmental fluctuations. Finally, their large population sizes mean that genetic diversity should be high, giving them high adaptive potential. This means that if any species can respond to rapid environmental change, it is likely A. tonsa.

# Experimental design
A. tonsa was collected from the wild, common gardened them for three generations, then split them into four treatments with four replicates each and about 3,000-5,000 individuals per replicate. there were left to evolve in these conditions for 25 generations.

Samples were collected at generation F0 and F25 to quantify methylation frequencies using reduced representation bisulfite sequencing (RRBS). The treatments were; * Control: ambient temp, ambient CO2 (AA) * ambient temp, high CO2 (AH) * high temp, ambient CO2 (HA) * high temp, high CO2 (HH).

# RRBS 
Following the adapter ligation, we bisulfite convert all unmethylated C’s to T’s.

Before starting we also spiked in a small amount of DNA from E. coli that we know wasn’t methylated. Using this, we can calculate downstream how efficient our bisulfite conversion was.

# Hypotheses and questions

# pipeline
## Visualize, clean, visualize (this step was already done)
* Visualize quality of raw data with fastqc
* clean raw data with Trimmomatic
* Visualize quality of cleaned data with fastqc
## Align to Acartia tonsa reference genome (Bismark)
* also align lambda DNA to check for conversion efficiency (Done for you)
## Extract methylation calls
## Process and filter calls
## Summary figures (PCA, clustering, etc)
## Test for differential methylation (Methylkit)

Take a look at the fastqc files before and after trimming
What do you notice that is different from fastqc that you’ve seen previously?

Why do we see these differences? - bisulfite conversion! And don’t forget that the reverse strand is the reverse complement of the bisulfite converted forward strand, and is not just the BS converted bottom strand.

# Align with Bismark
Why is this different from typical DNA alignment?

What do we need to do to the genome?

Now you want to run your assigned sample.
Remember, this is going to take at least 8 hours to run, so you’ll want to execute it in a screen.

```
#!/bin/bash

bismark --bowtie2 --multicore 1 \
    --genome /data/project_data/epigenetics/reference_genome \
    --output_dir /data/project_data/epigenetics/aligned_output \
    -1 /data/project_data/epigenetics/trimmed_fastq/SAMPLEID_1.fq.gz \
    -2 /data/project_data/epigenetics/trimmed_fastq/SAMPLEID_2.fq.gz \
    --rg_tag --rg_id SAMPLEID --rg_sample SAMPLEID --gzip --local --maxins 1000
 ```
 All you need to modify is the SAMPLEID information. For example, if your sample is HH_F25_3, then the forward read line would be: -1 /data/project_data/epigenetics/trimmed_fastq/HH_F25_3_1.fq.gz. And so on.

Bismark is, at its core, running bowtie2. But there are important differences. If you remember, we’re using two modified versions of the genome where we’ve converted C-to-T AND G-to-A. We also generate temporary files of our trimmed reads where we convert C-to-T (read1) AND G-to-A (read2) so they can map to this converted genome. This makes the alignment a bit harder because 1. the complexity of DNA is reduced, and 2. we are mapping to two separate genomes.

The parameters in our bismark aligment above specify the following:

--bowtie2 tells bismark to map with bowtie2. There are other options possible here (bowtie1, for example)
--multicore 1 Use one core. We could use multiple cores to make alignment faster, but we may crash the server if many of you are mapping at once if we do this.
--genome the location of our converted genome. The bismark package has a command to prep your genome that I've already done. You can check it out: bismark_methylation_extractor --help
-1 the path to your sample's forward read
-2 the path to your sample's reverse read
--rg_tag add a read group tag that identifies your individual sample in your output bam file
--rg_id the string that defines the readgroup ID
--rg_sample the string that defines your readgroup sample ID
--gzip for the temporary files, use gzip to save space
--local align with the local alignment option in bowtie2. This will include soft clipping, which should increase mapping rate, but comes at the cost of (maybe) increasing mis-mapping. 
--maxins specifies the maximum insert size for mapping. We're using 1000 as our libraries had a mean insert size of ~200. Note that this is smaller than most sequencing libraries.

Don’t forget to make your code executable chmod -u +x

When your job is done running. There should be 3 files output for your samples:

SAMPLEID_bismark_bt2_pe.bam the actual alignment
SAMPLEID_bismark_bt2_PE_report.txt text report of the alignment. Mapping success, etc.
SAMPLEID_bismark_bt2_PE_report.html html report of the alignment. You can download (with scp, etc) this and view in browser. It is pretty nice and nifty. Redundant info from the text file, but nicer to look at.

# Aligining Sample AA_F25_4 with Bismark

```
cd Ecological-Genomics-2020
cd myscripts
touch bismark.sh
vim bismark.sh
i

#!/bin/bash

bismark --bowtie2 --multicore 1 \
    --genome /data/project_data/epigenetics/reference_genome \
    --output_dir /data/project_data/epigenetics/aligned_output \
    -1 /data/project_data/epigenetics/trimmed_fastq/AA_F25_4_1.fq.gz \
    -2 /data/project_data/epigenetics/trimmed_fastq/AA_F25_4_2.fq.gz \
    --rg_tag --rg_id AA_F25_4 --rg_sample AA_F25_4 --gzip --local --maxins 1000

escape
:wq
enter

chmod u+x bismark.sh
ll
screen
bash bismark.sh

```



------    
<div id='id-section54'/>   


### Entry 54: 2020-03-26, Thursday.   



------    
<div id='id-section55'/>   


### Entry 55: 2020-03-27, Friday.   



------    
<div id='id-section56'/>   


### Entry 56: 2020-03-30, Monday.   



------    
<div id='id-section57'/>   


### Entry 57: 2020-03-31, Tuesday.   



------    
<div id='id-section58'/>   


### Entry 58: 2020-04-01, Wednesday.   

# LEARNING OBJECTIVES
* Extract methylation calls and assess raw data
* Visualize genome-wide patterns
* Identify differential methylation at the SNP level

# Extract methylation calls

Now extract methylation calls

bismark_methylation_extractor --bedGraph --scaffolds --gzip \
    --cytosine_report --comprehensive \
    --no_header \
    --parallel 6 \
    --output ~/tonsa_epigenetics/analysis/methylation_extract/ \
    --genome_folder /data/copepods/tonsa_genome/ \
    *pe.bam
    
Link to methylation extraction report

To ignore first two bases:

bismark_methylation_extractor --bedGraph --scaffolds --gzip \
    --cytosine_report --comprehensive \
    --no_header \
    --parallel 6 \
    --ignore 2 --ignore_r2 2 \
    --output ~/tonsa_epigenetics/analysis/methylation_extract/ \
    --genome_folder /data/copepods/tonsa_genome/ \
    *pe.bam
    
Link to methylation extraction report ignoring bias

* Column 1: Chromosome
* Column 2: Start position
* Column 3: End position
* Column 4: Methylation percentage
* Column 5: Number of methylated C's
* Column 6: Number of unmethylated C's
We can look at sites with some data using the following:

```
zcat HH_F25_4_1_bismark_bt2_pe.bismark.cov.gz | awk '$4 > 5 && $5 > 10' | head
```

# Analyze with methylkit
library(methylKit)
library(tidyverse)
library(ggplot2)
library(pheatmap)

# first, we want to read in the raw methylation calls with methylkit

# set directory with absolute path 
dir <- "/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020"

# read in the sample ids
samples <- read.table("~/Documents/GitHub/Ecological-Genomics-2020/sample_id.txt", header=FALSE)

# now point to coverage files
files <- file.path(dir, samples$V1)
all(file.exists(files)) # checking that all files exist

# convert to list
file.list <- as.list(files)

# get the names only for naming our samples
nmlist <- as.list(gsub("_1_bismark_bt2_pe.bismark.cov.gz","",samples$V1))

# use methRead to read in the coverage files

```
myobj <- methRead(location= file.list,
        sample.id =   nmlist,
                      assembly = "atonsa", # this is just a string. no actual database
                      dbtype = "tabix",
                      context = "CpG",
                      resolution = "base",
                      mincov = 20,
                            treatment = 
                              c(0,0,0,0,
                                1,1,1,1,
                                2,2,2,2,
                                3,3,3,3,
                                4,4,4,4),
                      pipeline = "bismarkCoverage",
                      dbdir = "~/Documents/GitHub/Ecological-Genomics-2020")
```
# Visualize coverage and filter

## We can look at the coverage for individual samples with getCoverageStats()
getCoverageStats(myobj[[1]], plot = TRUE, both.strands = FALSE) 
* Get CpG coverage information

## Also plot all of our samples at once to compare.

par(mfrow=c(1,1))

for(i in 1:length(myobj)) 
{  getCoverageStats(myobj[[i]], plot = TRUE, both.strands = FALSE) 
  * Get CpG coverage information} 
## Plot and save %CpG methylation information


# filter samples by depth with filterByCoverage()
filtered.myobj=filterByCoverage(myobj,lo.count=20,lo.perc=NULL,
                                      hi.count=NULL,hi.perc=97.5,
                                      db.dir = "~/Documents/GitHub/Ecological-Genomics-2020")
# merge samples
(Note! This takes a while and we're skipping it)

# use unite() to merge all the samples. We will require sites to be present in each sample or else will drop it

meth <- unite(filtered.myobj,mc.cores=3, suffix="united",
              db.dir = "~/Documents/UVM/Ecological_genomics_teaching/data/")
  * this requires a site to be present in all samples. This could be relaxed, depending on the application, with the min.per.group option.
  
# loading from databases
Methylkit has a convenient aspect where we can load previously generated databases. This means we don’t have to re-run analyses, but can skip ahead to where we previously left off. We will do this below to load the united database that I’ve already generated.
 
```
meth <- methylKit:::readMethylBaseDB(
                      dbpath = "~/Documents/UVM/Ecological_genomics_teaching/data/methylBase_united.txt.bgz",
                            dbtype = "tabix",
                            sample.id =   unlist(nmlist),
                            assembly = "atonsa", # this is just a string. no actual database
                            context = "CpG",
                            resolution = "base",
                            treatment = c(0,0,0,0,
                              1,1,1,1,
                              2,2,2,2,
                              3,3,3,3,
                              4,4,4,4),
                            destrand = FALSE)
```
# Methylation statistics across samples
## percMethylation() calculates the percent methylation for each site and sample

```
pm <- percMethylation(meth) # get percent methylation matrix

ggplot(gather(as.data.frame(pm)), aes(value)) + 
    geom_histogram(bins = 10, color="black", fill="grey") + 
    facet_wrap(~key) # can add  scales = 'free_x'

sp.means <- colMeans(pm)
p.df <- data.frame(sample=names(sp.means),
          group = substr(names(sp.means), 1,6),
          methylation = sp.means)

ggplot(p.df, aes(x=group, y=methylation, color=group)) + 
    stat_summary(color="black") + geom_jitter(width=0.1, size=3) 

```
# Summarize variation: PCA, clustering
```
clusterSamples(meth, dist="correlation", method="ward.D", plot=TRUE)

PCASamples(meth, screeplot=TRUE)
PCASamples(meth, screeplot=FALSE)
```
find differentially methylated sites between two groups

# subset with reorganize()

```
meth_sub <- reorganize(meth,  sample.ids= (c("AA_F00_1","AA_F00_2","AA_F00_3", "AA_F00_4",
                                          "HH_F25_1","HH_F25_2","HH_F25_3","HH_F25_4")), 
                              treatment=c(0,0,0,0,1,1,1,1),
                             save.db=FALSE)
```
                             
# calculate differential methylation

```
myDiff=calculateDiffMeth(meth_sub,
            overdispersion="MN",
            mc.cores=1,
            suffix = "AA_HH", adjust="qvalue",test="Chisq")
```

            
* where MN corrects for overdispersion
* fit a logistic regression to methylation values where explanatory variable is the treatment (case or control). 
* and we compare the fit of the model with explanatory variable vs the null model (no explanatory variable) 
and ask if the fit is better using a Chisq test. 
* the methylation proportions are weighted by their coverage, as in a typical logistic regression. Note that in theory you could enter these as two column success and failure data frame, which is common in logistic regressions.

* use overdispersion: Chisq without overdispersion finds more true positives, but many more false positives. good compromise is overdispersion with Chisq. reduced true pos, but really reduces false pos rate.

## get all differentially methylated bases
myDiff=getMethylDiff(myDiff,difference=10,qvalue=0.05)

## we can visualize the changes in methylation frequencies quickly.
hist(getData(myDiff)$meth.diff)

## get hyper methylated bases
hyper=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hyper")

## get hypo methylated bases
hypo=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hypo") 

# Plots of differentially methylated groups
* heatmaps first

## get percent methylation matrix
pm <- percMethylation(meth_sub)

## make a dataframe with snp id's, methylation, etc.
sig.in <- as.numeric(row.names(myDiff))
pm.sig <- pm[sig.in,]

## add snp, chr, start, stop

```
din <- getData(myDiff)[,1:3]
df.out <- cbind(paste(getData(myDiff)$chr, getData(myDiff)$start, sep=":"), din, pm.sig)
colnames(df.out) <- c("snp", colnames(din), colnames(df.out[5:ncol(df.out)]))
df.out <- (cbind(df.out,getData(myDiff)[,5:7]))
```

# heatmap

```
my_heatmap <- pheatmap(pm.sig,
        show_rownames = FALSE)

ctrmean <- rowMeans(pm.sig[,1:4])

h.norm <- (pm.sig-ctrmean)

my_heatmap <- pheatmap(h.norm,
        show_rownames = FALSE)
```

##### if you want to change colors. 

```
paletteLength <- 50
myColor <- colorRampPalette(c("cyan1", "black", "yellow1"))(paletteLength)
myBreaks <- c(seq(min(h.norm), 0, length.out=ceiling(paletteLength/2) + 1), 
              seq(max(h.norm)/paletteLength, max(h.norm), length.out=floor(paletteLength/2)))
              
my_heatmap <- pheatmap(h.norm,
        color=myColor, 
        breaks=myBreaks,
        show_rownames = FALSE)
```

# let's look at methylation of specific gene or snp

```
df.out
df.plot <- df.out[,c(1,5:12)] %>% pivot_longer(-snp, values_to = "methylation")
df.plot$group <- substr(df.plot$name,1,2)
head(df.plot)
```

## looking at snp LS049205.1:248
* if you choose a different snp, you can create different plots.
```
df.plot %>% filter(snp=="LS049205.1:248") %>% 
            ggplot(., aes(x=group, y=methylation, color=group, fill=group)) +
              stat_summary(fun.data = "mean_se", size = 2) +
              geom_jitter(width = 0.1, size=3, pch=21, color="black")
```

## write bed file for intersection with genome annotation

```
write.table(file = "~/Documents/UVM/Ecological_genomics_teaching/diffmeth.bed",
          data.frame(chr= df.out$chr, start = df.out$start, end = df.out$end),
          row.names=FALSE, col.names=FALSE, quote=FALSE, sep="\t")
```

# link to genes
in bash, on server.

```
/data/popgen/bedtools2/bin/bedtools closest -a diffmeth.bed \
      -b /data/project_data/epigenetics/GCA_900241095.1_Aton1.0_genomic.fa_annotation_table.bed \
      -D b | \
      awk '!($10=="-")' > hits.bed 
```

* the annotation file here has the format: ScaffoldName  FromPosition  ToPosition  Sense TranscriptName  TranscriptPath  GeneAccession GeneName  GeneAltNames  GenePfam  GeneGo  CellularComponent MolecularFunction BiologicalProcess

* note that the hits.bed file will paste the diffmeth.bed file before the annotation table. So the first 3 columns are fom diffmeth.bed, then next 8 from the annotation table.

## count up number of hits

cat hits.bed | wc -l

## count number of unique named genes
cat hits.bed | cut -f 8 | sort | uniq -c

## R Scripts for Assignment with treaments AA_F25 vs HA_F25 and AA_F25 vs HH_F25

```
library(methylKit)
library(tidyverse)
library(ggplot2)
library(pheatmap)

getwd() 
dir <- "/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020/Epigenetics_data"
samples <- read.table("~/Github/Ecological-Genomics-2020/Epigenetics_data/sample_id.txt", header=FALSE)
files <- file.path(dir, samples$V1)
all(file.exists(files))
file.list <- as.list(files)
nmlist <- as.list(gsub("_1_bismark_bt2_pe.bismark.cov.gz","",samples$V1))
myobj <- methRead(location = file.list,
                  sample.id =   nmlist,
                  assembly = "atonsa",
                  dbtype = "tabix",
                  context = "CpG",
                  resolution = "base",
                  mincov = 20,
                  treatment = 
                    c(0,0,0,0,
                      1,1,1,1,
                      2,2,2,2,
                      3,3,3,3,
                      4,4,4,4),
                  pipeline = "bismarkCoverage",
                  dbdir = "~/Documents/GitHub/Ecological-Genomics-2020/Epigenetics_data")
getCoverageStats(myobj[[1]], plot = TRUE, both.strands = FALSE)
par(mfrow=c(1,1))

meth <- methylKit:::readMethylBaseDB(
  dbpath = "/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020/Epigenetics_data/methylBase_united.txt.bgz",
  dbtype = "tabix",
  sample.id =   unlist(nmlist),
  assembly = "atonsa", 
  context = "CpG",
  resolution = "base",
  treatment = c(0,0,0,0,
                1,1,1,1,
                2,2,2,2,
                3,3,3,3,
                4,4,4,4),
  destrand = FALSE)
meth
pm <- percMethylation(meth)
ggplot(gather(as.data.frame(pm)), aes(value)) + 
  geom_histogram(bins = 10, color="black", fill="grey") + 
  facet_wrap(~key)

sp.means <- colMeans(pm)
dim(pm)
p.df <- data.frame(sample=names(sp.means),
                   group = substr(names(sp.means), 1,6),
                   methylation = sp.means)

ggplot(p.df, aes(x=group, y=methylation, color=group)) + 
  stat_summary(color="black") + geom_jitter(width=0.1, size=3) 

clusterSamples(meth, dist="correlation", method="ward.D", plot=TRUE)

PCASamples(meth, screeplot=TRUE)
PCASamples(meth, screeplot=FALSE)
```

# subset with AA_F25 and HA_F25
```
meth_sub <- reorganize(meth,  sample.ids= (c("AA_F25_1","AA_F25_2","AA_F25_3", "AA_F25_4",
                                             "HA_F25_1","HA_F25_2","HA_F25_3","HA_F25_4")), 
                       treatment=c(0,0,0,0,1,1,1,1),
                       save.db=FALSE)

myDiff=calculateDiffMeth(meth_sub,
                         overdispersion="MN",
                         mc.cores=1,
                         suffix = "AA_HA", adjust="qvalue",test="Chisq")
myDiff

myDiff <-getMethylDiff(myDiff,difference=10,qvalue=0.05)
myDiff

hist(getData(myDiff)$meth.diff)

hyper=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hyper")

hypo=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hypo")

pm <- percMethylation(meth_sub)

sig.in <- as.numeric(row.names(myDiff))
pm.sig <- pm[sig.in,]

din <- getData(myDiff)[,1:3]
df.out <- cbind(paste(getData(myDiff)$chr, getData(myDiff)$start, sep=":"), din, pm.sig)
colnames(df.out) <- c("snp", colnames(din), colnames(df.out[5:ncol(df.out)]))
df.out <- (cbind(df.out,getData(myDiff)[,5:7]))

my_heatmap <- pheatmap(pm.sig,
                       show_rownames = FALSE)
ctrmean <- rowMeans(pm.sig[,1:4])

h.norm <- (pm.sig-ctrmean)

my_heatmap <- pheatmap(h.norm,
                       show_rownames = FALSE)

df.out
df.plot <- df.out[,c(1,5:12)] %>% pivot_longer(-snp, values_to = "methylation")
df.plot$group <- substr(df.plot$name,1,2)
head(df.plot)

df.plot %>% filter(snp=="LS043685.1:10221") %>% 
  ggplot(., aes(x=group, y=methylation, color=group, fill=group)) +
  stat_summary(fun.data = "mean_se", size = 2) +
  geom_jitter(width = 0.1, size=3, pch=21, color="black")

## write bed file for intersection with genome annotation

write.table(file = "/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020/Epigenetics_data/diffmeth.bed",
            data.frame(chr= df.out$chr, start = df.out$start, end = df.out$end),
            row.names=FALSE, col.names=FALSE, quote=FALSE, sep="\t")
```

# subset with AA_F25 and HH_F25
```
meth_sub <- reorganize(meth,  sample.ids= (c("AA_F25_1","AA_F25_2","AA_F25_3", "AA_F25_4",
                                             "HH_F25_1","HH_F25_2","HH_F25_3","HH_F25_4")), 
                       treatment=c(0,0,0,0,1,1,1,1),
                       save.db=FALSE)

myDiff=calculateDiffMeth(meth_sub,
                         overdispersion="MN",
                         mc.cores=1,
                         suffix = "AA_HA", adjust="qvalue",test="Chisq")
myDiff

myDiff <-getMethylDiff(myDiff,difference=10,qvalue=0.05)
myDiff

hist(getData(myDiff)$meth.diff)

hyper=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hyper")

hypo=getMethylDiff(myDiff,difference=10,qvalue=0.05,type="hypo")

pm <- percMethylation(meth_sub)

sig.in <- as.numeric(row.names(myDiff))
pm.sig <- pm[sig.in,]

din <- getData(myDiff)[,1:3]
df.out <- cbind(paste(getData(myDiff)$chr, getData(myDiff)$start, sep=":"), din, pm.sig)
colnames(df.out) <- c("snp", colnames(din), colnames(df.out[5:ncol(df.out)]))
df.out <- (cbind(df.out,getData(myDiff)[,5:7]))

my_heatmap <- pheatmap(pm.sig,
                       show_rownames = FALSE)
ctrmean <- rowMeans(pm.sig[,1:4])

h.norm <- (pm.sig-ctrmean)

my_heatmap <- pheatmap(h.norm,
                       show_rownames = FALSE)

df.out
df.plot <- df.out[,c(1,5:12)] %>% pivot_longer(-snp, values_to = "methylation")
df.plot$group <- substr(df.plot$name,1,2)
head(df.plot)

df.plot %>% filter(snp=="LS070286.1:6277") %>% 
  ggplot(., aes(x=group, y=methylation, color=group, fill=group)) +
  stat_summary(fun.data = "mean_se", size = 2) +
  geom_jitter(width = 0.1, size=3, pch=21, color="black")

## write bed file for intersection with genome annotation

write.table(file = "/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020/Epigenetics_data/diffmeth.bed",
            data.frame(chr= df.out$chr, start = df.out$start, end = df.out$end),
            row.names=FALSE, col.names=FALSE, quote=FALSE, sep="\t")
```

------    
<div id='id-section59'/>   


### Entry 59: 2020-04-02, Thursday.   



------    
<div id='id-section60'/>   


### Entry 60: 2020-04-03, Friday.   



------    
<div id='id-section61'/>   


### Entry 61: 2020-04-06, Monday.   



------    
<div id='id-section62'/>   


### Entry 62: 2020-04-07, Tuesday.   



------    
<div id='id-section63'/>   


### Entry 63: 2020-04-08, Wednesday.   



------    
<div id='id-section64'/>   


### Entry 64: 2020-04-09, Thursday.   



------    
<div id='id-section65'/>   


### Entry 65: 2020-04-10, Friday.   



------    
<div id='id-section66'/>   


### Entry 66: 2020-04-13, Monday.   

# PRESENTATION OF PROJECT PROPOSAL

* Question: does an organism's nutritional status (diet) influence epigenetic response to insecticide induced stress in Colorado potato Beetle (CPB) *Leptinotarsa decemlienata

* Background: epigentic marks are responsive to changes in diet, environment and xenobiotic stress. the study examines how nutritional factors in combination with toxic stress influence patterns of DNA methylation in CPB, a devastating pest of solanaceae crops. Dietary supplements; Vitamin B12 and Methionine building blocks required for DNA methlation maintenance were chosen to test that nutritional status of an organism may have an impact on epigentic responses to toxic stress.

* Hypothesis: toxic stress will limit the synthesis of methyl donors resulting in lower levels of global methylation. nutritional supplements associated with DNA methylation may ameliorate oxidative stress response to xenobiotic exposure (insecticide)

* Method: female beetles collected from organic farm in VT, Potato leaf discs treated with either water (control) or imidacloprid (treatment) then each treatment further administered Vit B12, methionine or water.
Larvae weighed before and after 24hrs of receiving treatment.
 *Genomic data isolate N=18, n= 3 larvae/group
 *whole genome bisulphite sequencing on illumina
 *FastQC used to assess quality of raw reads, trimming done with Trimgalore
 *trimmed reads mapped to CPB reference genome using BISMARK


------    
<div id='id-section67'/>   


### Entry 67: 2020-04-14, Tuesday.   



------    
<div id='id-section68'/>   


### Entry 68: 2020-04-15, Wednesday.   



------    
<div id='id-section69'/>   


### Entry 69: 2020-04-16, Thursday.   



------    
<div id='id-section70'/>   


### Entry 70: 2020-04-17, Friday.   



------    
<div id='id-section71'/>   


### Entry 71: 2020-04-20, Monday.   

# Project analysis with methylKit

```
getwd()
setwd("./")
library(methylKit)
library(grid)
library(readr)
library(ggplot2)

dir<- "C:/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020/Beetle data"


samples<- read.table("C:/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020/Beetle data/sample_id.txt", header = FALSE)

samples

files<- file.path(dir, samples$V1)
all(file.exists(files))

file.list<- as.list(files)


nmlist<- as.list(gsub("_R1_001_val_1_bismark_bt2_pe.bismark.cov.gz", "", samples$V1))

```

# nmlist_2 <- nmlist[c("CC1","CC2", "CC3", "MC1", "MC2", "MC3", "BC1", "BC2", "BC3", "CM1", "CM2", "CM3", "MI1", "MI2", "MI3", "BI1", "BI2", "BI3")]


                              
```
nmlist_2 <- nmlist[c("CC1","CC2", "CC3", "MC1", "MC2", "MC3", "BC1", "BC2", "BC3", "CM1", "CM2", "CM3", "MI1", "MI2", "MI3", "BI1", "BI2", "BI3")]

myobj <- methRead(
  location = file.list,
  sample.id = nmlist,
  assembly = "Ldec",
  dbtype = "tabix",
  context = "CpG",
  resolution = "base", 
  mincov = 10, 
  treatment = c(1,1,1,
                2,2,2,
                0,0,0,
                3,3,3,
                4,4,4,
                5,5,5),
  pipeline = "bismarkCoverage",
  dbdir = "C:/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020/Beetle data")


pdf("initial_look.pdf") 
getMethylationStats(myobj[[1]],plot=TRUE,both.strands=FALSE)
dev.off()

pdf("initial_look2.pdf") 
getCoverageStats(raw_data[[2]],plot=TRUE,both.strands=FALSE)
dev.off()
```
```
getCoverageStats(myobj[[1]], plot = TRUE) 
getCoverageStats(myobj[[2]], plot = TRUE) 
getCoverageStats(myobj[[3]], plot = TRUE) 
getCoverageStats(myobj[[4]], plot = TRUE) 
getCoverageStats(myobj[[5]], plot = TRUE)
getCoverageStats(myobj[[6]], plot = TRUE) 
getCoverageStats(myobj[[7]], plot = TRUE) 
getCoverageStats(myobj[[8]], plot = TRUE)
getCoverageStats(myobj[[9]], plot = TRUE) 
getCoverageStats(myobj[[10]], plot = TRUE) 
getCoverageStats(myobj[[11]], plot = TRUE)
getCoverageStats(myobj[[12]], plot = TRUE) 
getCoverageStats(myobj[[13]], plot = TRUE) 
getCoverageStats(myobj[[14]], plot = TRUE)
getCoverageStats(myobj[[15]], plot = TRUE) 
getCoverageStats(myobj[[16]], plot = TRUE) 
getCoverageStats(myobj[[17]], plot = TRUE)
getCoverageStats(myobj[[18]], plot = TRUE)

getMethylationStats(myobj[[11]],plot=FALSE,both.strands=FALSE)

Filter data by coverage; must have at least 10 reads per CpG and not exceed the 99.9th percentile (PCR duplicates), also tested with only 4 reads and similar results in terms of the number of diff methylated CpGs.
```{r Filtering for Coverage}
filtered_data <- filterByCoverage(myobj,lo.count=10,lo.perc=NULL,
                                  hi.count=NULL,hi.perc=99.9)
```

Select only CpGs found in all 6 samples for analysis (destand =T can be used for CpG methylation to use both strands and incresae coverage, this should not be used for non-CpG though). You can decrease the minimum number of samples a particular CpG occurs in by using 'min.per.group =2L'.
```{r Combine Data}
meth_all_data <- unite(filtered_data, destrand=TRUE)
```
```
meth<-methylKit::unite(filtered_data, mc.cores= 1, min.per.group = NULL, suffix= "united")
```

Filter the object above so we only take CpGs that have at least 25% methylation in at least one sample for analysis: this massively reduces the load for the multiple correction testing.
```{r Filter for Percent Cs per Position}

df_meth_all <- getData(meth)
df_meth_all$percent_1 <-(df_meth_all$numCs1/df_meth_all$coverage1)*100
df_meth_all$percent_2 <-(df_meth_all$numCs2/df_meth_all$coverage2)*100
df_meth_all$percent_3 <-(df_meth_all$numCs3/df_meth_all$coverage3)*100
df_meth_all$percent_4 <-(df_meth_all$numCs4/df_meth_all$coverage4)*100
df_meth_all$percent_5 <-(df_meth_all$numCs5/df_meth_all$coverage5)*100
df_meth_all$percent_6 <-(df_meth_all$numCs6/df_meth_all$coverage6)*100
df_meth_all$percent_7 <-(df_meth_all$numCs7/df_meth_all$coverage7)*100
df_meth_all$percent_8 <-(df_meth_all$numCs8/df_meth_all$coverage8)*100
df_meth_all$percent_9 <-(df_meth_all$numCs9/df_meth_all$coverage9)*100
df_meth_all$percent_10<-(df_meth_all$numCs10/df_meth_all$coverage10)*100
df_meth_all$percent_11 <-(df_meth_all$numCs11/df_meth_all$coverage11)*100
df_meth_all$percent_12 <-(df_meth_all$numCs12/df_meth_all$coverage12)*100
df_meth_all$percent_13 <-(df_meth_all$numCs13/df_meth_all$coverage13)*100
df_meth_all$percent_14 <-(df_meth_all$numCs14/df_meth_all$coverage14)*100
df_meth_all$percent_15 <-(df_meth_all$numCs15/df_meth_all$coverage15)*100
df_meth_all$percent_16 <-(df_meth_all$numCs16/df_meth_all$coverage16)*100
df_meth_all$percent_17 <-(df_meth_all$numCs17/df_meth_all$coverage17)*100
df_meth_all$percent_18 <-(df_meth_all$numCs18/df_meth_all$coverage18)*100


subset_meth <- subset(df_meth_all, percent_1 >= 25 | percent_2 >= 25 | percent_3 >= 25 |
                        percent_4 >= 25 | percent_5 >= 25 | percent_6 >= 25 | percent_7 >= 25 | percent_8 >= 25| percent_9 >= 25 | percent_10 >= 25 | percent_11 >= 25 | percent_12 >= 25 |percent_13 >= 25 | percent_14 >= 25 | percent_15 >= 25 | percent_16 >= 25 | percent_17 >= 25 | percent_18 >= 25) 

row_numbers <- as.vector(row.names(subset_meth))
row_numbers <- as.numeric(row_numbers) 
subset_methBase <- select(meth, row_numbers)
```

# end of filtering 
write.table(subset_methBase, file="subset_meth_25.txt")

#correlation plots -takes a long time to plot

# getCorrelation(meth,plot=TRUE)
clusterSamples(subset_methBase, dist="correlation", method="ward", plot=F)
#clustering by methylation similarity
PCASamples(subset_methBase, screeplot=F) 
#samples are clustering together..

#calculate percent meth using the subsetted data with all samples

pm<-percMethylation(subset_methBase)
tot<- colSums(pm) #total methylated C's

#plotting distribution of per meth values
ggplot(gather(as.data.frame(pm)), aes(value)) + 
  geom_histogram(bins = 10, color="black", fill="grey") + 
  facet_wrap(~key)

ggplot(gather(as.data.frame(pm)), aes(value)) + geom_histogram(bins = 10, colors="black", fill="grey") + facet_wrap(~key)
par(mfrow=c(1,1))
sp.means<- colMeans(pm) #colmeans- column means of per meth for each sample


#reading in merged file 
meth <- methylKit:::readMethylBaseDB(
  dbpath ="C:/Users/sandr/OneDrive/Documents/GitHub/Ecological-Genomics-2020/Beetle data/methylDB 2020-04-19 xAJ/methylBase_united.txt.bgz",
  dbtype = "tabix",
  sample.id = unlist(nmlist),
  assembly = "Ldec", 
  context = "CpG",
  resolution = "base",
  treatment = c(
    1,1,1,
    2,2,2,
    0,0,0,
    3,3,3,
    4,4,4,
    5,5,5),
  destrand = FALSE)

#plotting by mean per meth/tot meth Cs
tot<- colSums(pm) #total methylated C's

sample= names(sp.means)
group= c("Control", "Control", "Control", "Insecticide", "Insecticide", "Insecticide", "Control", "Control", "Control", "Insecticide", "Insecticide", "Insecticide", "Control", "Control", "Control", "Insecticide", "Insecticide", "Insecticide" )

treat= c("B12", "B12", "B12", "B12", "B12", "B12", "control", "control", "control", "control", "control", "control", "methionine","methionine", "methionine","methionine","methionine","methionine")
methylation = sp.means

pf<-data.frame(sample, treat, group, tot)

write.table(pf, file="pf.txt")
# library(ggpubr)
# library(betareg)

p<-ggplot(pf, aes(x=treat, y=methylation, fill= treat)) +  geom_point(aes(color= group)) + labs(x= "Group", y="% methylation", title ="Mean % methylation by treatment ") + theme_bw() + theme(text=element_text(size=14)) 
p 
#a lot of overlap among means

p<-ggplot(pf, aes(x=treat, y=tot, fill= group)) +  geom_boxplot() + labs(x= "Group", y="methylated C's", title ="total no. methylated C's") + theme_bw() + theme(text=element_text(size=14))
p

my_test<-summary(aov(tot~ treat*group, pf)) 

#contrast Control (water-water) vs Imidaclorprid-control

meth_sub_CvCM <- reorganize(subset_methBase, sample.ids = c("CC1","CC2", "CC3", "CM1", "CM2", "CM3"), 
                            treatment = c(0,0,0,3,3,3),
                            save.db = FALSE)

meth_sub_CvCM

#as in ecol genomics- calculate diff methylation for this contrast
myDiff<- calculateDiffMeth(meth_sub_CvCM, overdispersion = "MN", mc.cores = 1, suffix= "CC_CM",
                           adjust = "qvalue",
                           test = "Chisq")

# as in marshall et al- no overdispersion correction...
diff_meth_bees<- calculateDiffMeth(meth_sub_CvCM, mc.cores = 1, suffix= "CC_CM")
getMethylDiff(diff_meth_bees,difference=5,qvalue=0.05) #without correcting for overdispersion as in marshall et al
which(getData(diff_meth_bees)$qvalue < 0.05) #51 diff snps
min(getData(diff_meth_bees)$qvalue)


#follwing the ecol genomics tutorial
# get all differentially methylated bases

myDiff=getMethylDiff(myDiff,difference=10,qvalue=0.05) #3 diff snps

# myDiff_1<- getMethylDiff(myDiff_1, qvalue= 0.05, difference=10)
dim(myDiff)
getData(myDiff)$qvalue
which(getData(myDiff)$qvalue < 0.05)
min(getData(myDiff)$qvalue)


# hist(getData(myDiff)$meth.diff)

# hyper=getMethylDiff(myDiff,difference=10,qvalue=0.01,type="hyper")
# hypo=getMethylDiff(myDiff_1,difference=10,qvalue=0.01,type="hypo")
#percent meth of contrast
pm_1 <- percMethylation(meth_sub_CvCM)
# tot_cvsi<- colSums(pm_1)
#
#############ploting total no. of meth C's by group
# sample_cvsi= colnames(pm_1)
# group= c("Control", "Control", "Control", "Insecticide", "Insecticide", "Insecticide")
# 
# total = tot_cvsi
# 
# pf_cvsi<-data.frame(sample_cvsi, group, total)
# p_cvsi<-ggplot(pf_cvsi, aes(x=group, y=total, fill= group)) +  geom_boxplot() + labs(x= "Group", y="methylated C's", title ="total no. methylated C's") + theme_bw() + theme(text=element_text(size=14))
# p_cvsi
################done

# head(pm_1)
# dim(pm_1)
write.table(pm_1, file="pm_CvsI.txt")



sig.in <- as.numeric(row.names(myDiff))
pm.sig <- pm[sig.in]
#pm_1[sig.in_1, ] : subscript out of bounds
# PCASamples(subset_methBase)

library(pheatmap)

#heat map wont work bc pm.sig isnt running
require(pheatmap)
my_heatmap <- pheatmap(pm.sig, show_rownames = FALSE) #need to run on terminal

#normalizing 
ctrmean_1 <- rowMeans(pm.sig_1[,1:3])
h.norm_1<- (pm.sig_1-ctrmean_1) 

my_heatmap_1 <- pheatmap(h.norm_1,
                         show_rownames = FALSE)

#also not working
din_1 <- getData(myDiff)[,1:3]
df.out_1 <- cbind(paste(getData(myDiff)$chr, getData(myDiff)$start, sep=":"), din_1, pm.sig_1)
colnames(df.out_1) <- c("snp", colnames(din_1), colnames(df.out_1[5:ncol(df.out_1)]))
df.out_1 <-(cbind(df.out_1,getData(myDiff_1)[,5:7]))

df.plot_1 <-df.out_1[ ,c(1,5:10)] %>% pivot_longer(-snp, values_to = "methylation")
df.plot_1$group <- substr(df.plot_1$name,1,2)

head(df.plot_1)

write.table(df.plot_1, file="dfCvsI.txt")

# plotting diff meth snps- ignore
one<- df.plot_1 %>% filter(snp=="AYNB02000327.1:1877") %>% 
  ggplot(., aes(x=group, y=methylation, color=group, fill=group)) +
  stat_summary(fun.data = "mean_se", size = 1) +
  geom_jitter(width = 0.1, size=3, pch=21) +
  theme_classic() +
  labs(title = "Control vs. Inseciticide", x= "Treatment", y= "% methylated sites")  + theme(text=element_text(size=12), legend.position ="none")


########################################################################################################
#contrast Control (water-water) vs B12-control

meth_sub_CvBC <- reorganize(subset_methBase, sample.ids = c("CC1","CC2", "CC3", "BC1", "BC2", "BC3"), 
                            treatment = c(0,0,0,1,1,1),
                            save.db = FALSE)

meth_sub_CvBC

#as in ecol genomics- calculate diff methylation for this contrast
myDiff_CvBC<- calculateDiffMeth(meth_sub_CvBC, overdispersion = "MN&quo...

* apply to different contrasts and save txt files
------    
<div id='id-section72'/>   


### Entry 72: 2020-04-21, Tuesday.   



------    
<div id='id-section73'/>   


### Entry 73: 2020-04-22, Wednesday.   



------    
<div id='id-section74'/>   


### Entry 74: 2020-04-23, Thursday.   



------    
<div id='id-section75'/>   


### Entry 75: 2020-04-24, Friday.   



------    
<div id='id-section76'/>   


### Entry 76: 2020-04-27, Monday.   



------    
<div id='id-section77'/>   


### Entry 77: 2020-04-28, Tuesday.   



------    
<div id='id-section78'/>   


### Entry 78: 2020-04-29, Wednesday.   

# Final Project Presentation


------    
<div id='id-section79'/>   


### Entry 79: 2020-04-30, Thursday.   



------    
<div id='id-section80'/>   


### Entry 80: 2020-05-01, Friday.   



------    
<div id='id-section81'/>   


### Entry 81: 2020-05-04, Monday.   



------    
<div id='id-section82'/>   


### Entry 82: 2020-05-05, Tuesday.   



------    
<div id='id-section83'/>   


### Entry 83: 2020-05-06, Wednesday.   



------    
<div id='id-section84'/>   


### Entry 84: 2020-05-07, Thursday.   



------    
<div id='id-section85'/>   


### Entry 85: 2020-05-08, Friday.   



